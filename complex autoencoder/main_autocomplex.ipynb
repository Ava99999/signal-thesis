{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e465fce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import scipy\n",
    "import random\n",
    "import cmath\n",
    "#import pylops # might not need\n",
    "import math\n",
    "import pyproximal\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist, mnist\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.saving import register_keras_serializable, deserialize_keras_object\n",
    "from tensorflow.test import compute_gradient\n",
    "from tensorflow.compat.v1 import assign_sub\n",
    "\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "from scipy.fft import fft, ifft, fft2, ifft2\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import autosetup \n",
    "from backpropagation import CBP, CBP_decoder\n",
    "from complex_optimizer import Complex_SGD, adaptive_stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6243d911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed190db",
   "metadata": {},
   "source": [
    "Some nice tensorflow links\n",
    "https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing#putting_it_all_together_an_end-to-end_example \n",
    "https://www.tensorflow.org/guide/keras/functional_api\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Layer#used-in-the-notebooks\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Layer#call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64642c61",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00bbead",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae6f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from: https://www.tensorflow.org/tutorials/generative/autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1653b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000, 10, 10)\n",
      "(10000, 10, 10)\n",
      "(60000, 10, 10)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "10000\n",
      "tf.Tensor(\n",
      "[[1.+0.j         1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j        ]\n",
      " [1.+0.j         1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j        ]\n",
      " [1.+0.j         1.+0.j         1.+0.2637255j  1.+0.24254899j\n",
      "  1.+0.j         1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j        ]\n",
      " [1.+0.j         1.+0.j         1.+0.24843131j 1.+0.35329404j\n",
      "  1.+0.7608235j  1.+0.7760783j  1.+0.68403924j 1.+0.75254905j\n",
      "  1.+0.j         1.+0.j        ]\n",
      " [1.+0.j         1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j         1.+0.6838421j  1.+0.15509808j\n",
      "  1.+0.j         1.+0.j        ]\n",
      " [1.+0.j         1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j         1.+0.8299611j  1.+0.j\n",
      "  1.+0.j         1.+0.j        ]\n",
      " [1.+0.j         1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.7741169j  1.+0.11117707j 1.+0.j\n",
      "  1.+0.j         1.+0.j        ]\n",
      " [1.+0.j         1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.16196033j 1.+0.62196094j 1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j        ]\n",
      " [1.+0.j         1.+0.j         1.+0.j         1.+0.02152937j\n",
      "  1.+0.99607843j 1.+0.02039224j 1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j        ]\n",
      " [1.+0.j         1.+0.j         1.+0.j         1.+0.12811767j\n",
      "  1.+0.66388243j 1.+0.j         1.+0.j         1.+0.j\n",
      "  1.+0.j         1.+0.j        ]], shape=(10, 10), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255. #normalize the data\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# make a smaller training set\n",
    "x_train_temp = x_train[..., tf.newaxis]\n",
    "x_train_temp = tf.image.resize(x_train_temp, [10,10])\n",
    "x_train_small = x_train_temp[:,:,:,0]\n",
    "\n",
    "x_test_temp = x_test[..., tf.newaxis]\n",
    "x_test_temp = tf.image.resize(x_test_temp, [10,10])\n",
    "x_test_small = x_test_temp[:,:,:,0]\n",
    "\n",
    "#x_train_small = tf.image.resize(x_train, [10, 10])\n",
    "#x_test_small = tf.image.resize(x_test, [10, 10])\n",
    "\n",
    "print(x_train_small.shape)\n",
    "print(x_test_small.shape)\n",
    "\n",
    "# make a complex (smaller) training set\n",
    "x_train_cx_small = tf.complex(np.ones((x_train_small.shape)).astype('float32'), x_train_small)\n",
    "x_test_cx_small = tf.complex(np.ones((x_test_small.shape)).astype('float32'), x_test_small)\n",
    "\n",
    "# make the training set smaller for testing code\n",
    "x_train_cx_reduced = x_train_cx_small[0:10000, :, :]\n",
    "\n",
    "print(x_train_cx_small.shape)\n",
    "print(type(x_train_cx_small))\n",
    "\n",
    "print(x_train_cx_reduced.shape[0])\n",
    "\n",
    "print(x_test_cx_small[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3324ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183, 64)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "(183, 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAFBCAYAAAAR9FlyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAANxUlEQVR4nO3df6zd9V3H8e+593ChpZcOZC25tqXbuqqFqJC5ATrmgk7aLCFkmyNsf2gXZC4xtMNlP/5QZ6KY+KMsixAyFcMmWeJCR0xYzMRYRtaxEBfdirY22eBmF291M+0tcPvjnmNuTYl/kPR83733rud1Ho/kcvnjvvl8b9/ne+6TU8rp9Pv9fgMAwFAb+1FfAAAA50/UAQAEEHUAAAFEHQBAAFEHABBA1AEABBB1AAABRB0AQIDuIF/U6/WamZmZZnJysul0Ost/VSyJxf+v9NzcXDM1NdWMjbXrdzsfTnY+eux89Nj56OkPuPOBom7xAbBx48alvD5W0PT0dLNhw4ZWM3Y+3Ox89Nj56LHz0TN9jp0PFHWLRb/o+X/e3Fy2xu/YDotjx3vN1dd/79X9tWHnw8nOR4+djx47Hz3HBtz5QFF39iXaxQfAZZMeBMOm8hK7nQ83Ox89dj567Hz0dM6xcxsFAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACdAf5on6/f+bzseO95b4eltDZfZ3dXxt2PpzsfPTY+eix89FzbMCdDxR1c3NzZz5fff33luLaWGGL+1u7dm3rmUV2PpzsfPTY+eix89Ezd46dd/oDpH6v12tmZmaaycnJptPpLPU1skwWV7v4AJiammrGxtr9TrudDyc7Hz12PnrsfPT0B9z5QFEHAMCFzR+UAAAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACNAd5It6vV4zMzPTTE5ONp1OZ/mviiXR7/ebubm5Zmpqqhkba9fvdj6c7Hz02PnosfPR0x9w5wNF3eIDYOPGjUt5fayg6enpZsOGDa1m7Hy42fnosfPRY+ejZ/ocOx8o6haLftEvNDuabnNRs9ymP/m20twXP/iZ0tyXj/1Mae4bt21qPbNw5L+alXK6OdU83Tzx6v7aOJ+dj//Um1ufd/C31jQVe9/xYGnu30+uK819cv97Ws+88dGF0lljT//r8Ox83etbn/cfH93cVNz0c/9Wmrv3qn9YscfKZ3/nV0tnrf67Z4dm5xWHH7iuNPftX3qkNPcnP9hSmnvsC7/Yemb9nz/TrJRhus9f+LMrm4pHr3u4NHfP4feV5lbdNR/x83ygqDv7Eu3iA6DbWf4bf/ySS0pzayZr/4ngJb3a99Qdm2g901mBX79X9c+e2f4l9vPZ+fj4xa3PG1u1sjtffXK8NFe5zm63GHWVx8qPaueFe2GseJ9PrGl/1ko/VroX1b637hDtvKJ6n19WfW4/Ufuexi8u3Oej8NxeuM/HV7f/eXA+92v30tp53bFexM9zf1ACACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAIM9N6v5+Pwnhtaz3zolidLZ73v/o+V5v521x+X5r5yU/s3fV6990iTbm7r61rP3PWWfyqdtf2xe0tzvbWnS3PfvfUvWs+86eiHS2dt2dcMjVM/+eOtZzZd+2LprKf2X1Oba2pzN994oPXMlo8/VzprZm8T7bJv1963961P/WZp7pt/+GBp7lMfO9h65lf2/GyTrnKfz79Qey/W7S/Unts3FZ9X1j/+SuuZmfZ5s+y8UgcAEEDUAQAEEHUAAAFEHQBAAFEHABBA1AEABBB1AAABRB0AQABRBwAQQNQBAAQQdQAAAUQdAECA7nIf8BOf+2Hrmaf/6OrSWe//6pOluS8dvb40t3rvM6W5dJVfl317V5XOWrO79u8ln/7IF0tzh0691HrmjV8+0aQb2/et1jMT+2pnbWmeL82Nb9tamnvzLUdaz3z+sVtKZ21qvt4ku2pP7fs7vOeGFbtfF33wd3+79czlzf4mXeU+31K8z//n124szd3z7loHbJuYbT2za/3tpbMWZts/pwzKK3UAAAFEHQBAAFEHABBA1AEABBB1AAABRB0AQABRBwAQQNQBAAQQdQAAAUQdAEAAUQcAEEDUAQAEEHUAAAG6y33AwnOHWs+Mb9taOuu9a/eW5u4+dGdpbtX6V1rPLMweKZ3Fa7tix/dX9LxdO3a2nhk78K1luZZRVX1+uP8rD5fm7nvx1tYzb3jwcOmshdJUvh/7l05p7rl3ry/Nrf3ufGmOpXPpB2ZKc9smZlfsuX1h9mBzofFKHQBAAFEHABBA1AEABBB1AAABRB0AQABRBwAQQNQBAAQQdQAAAUQdAEAAUQcAEEDUAQAEEHUAAAFEHQBAgG5zAVp47lBpbtdbby/NbX78h6W55vH2I7O3rSsdtTB7pDSXbtWuS0pz256YLc29cv9865mJXy4dxRI/P3zp6PWlue88dG3rmStm95fO4rVd/te1X8/dN99Rmvv7Rz7TembXjp2lsxYOHCzNpTv9wFW1wfubkeaVOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAAog4AIICoAwAIIOoAAAKIOgCAAN0myMLskdLc7G3rSnPPP/D61jPzn5gsnbVld+17S7dw4GBpbteOnaW5h574q9YzH7r9o6WzVu99pjTHa/vcs28vzd21+x9bz+x7eFXpLJbW1p3Plubu/uqdrWc2/+VM6ayZG0pj8arPf/d9/NbS3PO/3z6HNrynueB4pQ4AIICoAwAIIOoAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgAggKgDAAgg6gAAAog6AIAA7d/BdgUc3lN7h+Opp/qlufnX1dr2D376861ndh+9o0k3vn5d65n/3v6m0lknLu+U5t7/60+W5rZedGnrmWObx0tnrW6yvfB7N5XmTq7tleb2vP0LpbltE7OtZ/Y1P186K93Lt7+tNDdzc+0+33Tti6W5h7Y+2nrmvhdrbySfrveO60pz81dMlOa++f3jpbkDN/5N65kd699VOmth9kizXLxSBwAQQNQBAAQQdQAAAUQdAEAAUQcAEEDUAQAEEHUAAAFEHQBAAFEHABBA1AEABBB1AAABRB0AQABRBwAQoNtcgCaO1lpz+6efbFbS7q/d0Xpm685nm3hXXt565JoPf6cZBu88cFvrmav2fH1ZrmXY3feBR0pz2yZmS3N3H7qzNLfqnosLU4dKZ6XrfuQ/S3N/urn23P74D64rzW1/7N7WM1t2f6N0Vrp3frb2/PepKw+W5g6deqk0d83+32g9s2H2QHOh8UodAEAAUQcAEEDUAQAEEHUAAAFEHQBAAFEHABBA1AEABBB1AAABRB0AQABRBwAQQNQBAAQQdQAAAbqDfFG/3z/z+XRzqmn+72+X1cL8fGlu/vipZiX1Xml/naf7K3eNZ/b1//bXxvnsvL9wovV5J4+fbFbSfK+2h9Mvtf/exkZg5xUvzy2U5o5P9FZsd2fmCpe5YOdLuoPqY6X6vNIr/Awahef2lfy5fOzi2n1+/FRtbuHlExk77w9genp68Z/iY0g/FvfXlp0P94edj96HnY/eh52P3sf0OXbeWfzLuQqx1+s1MzMzzeTkZNPpdJasPFlei6udm5trpqammrGxdr/TbufDyc5Hj52PHjsfPf0Bdz5Q1AEAcGHzByUAAAKIOgCAAKIOACCAqAMACCDqAAACiDoAgACiDgCgGX7/Cxtn03kogDkHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load MNIST dataset from ptygenography instead\n",
    "X_pty = np.load('MNIST_64_only3.npy')\n",
    "n = X_pty.shape[1]\n",
    "nx = int(np.sqrt(n))\n",
    "print(X_pty.shape)\n",
    "\n",
    "X_tf = tf.convert_to_tensor(X_pty, dtype=tf.complex64)\n",
    "print(type(X_tf))\n",
    "print(X_tf.shape)\n",
    "\n",
    "# show the data\n",
    "ns = 5\n",
    "fig, ax = plt.subplots(2,ns)\n",
    "\n",
    "for i in range(ns):\n",
    "    x = X_pty[i]\n",
    "    ax[0,i].imshow(np.real(x).reshape((nx,nx)),clim=[0,1])\n",
    "    ax[0,i].set_xticks([])\n",
    "    ax[0,i].set_yticks([])\n",
    "    ax[1,i].imshow(np.imag(x).reshape((nx,nx)),clim=[0,1])\n",
    "    ax[1,i].set_xticks([])\n",
    "    ax[1,i].set_yticks([])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f5faaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# representation training dataset (real case)\n",
    "ns = 5\n",
    "fig, ax = plt.subplots(1,ns)\n",
    "\n",
    "for i in range(ns):\n",
    "    x = x_train_small[i]\n",
    "    ax[i].imshow(np.real(x).reshape((10,10)),clim=[0,1])\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acad1eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ns = 5\n",
    "fig, ax = plt.subplots(2,ns)\n",
    "\n",
    "for i in range(ns):\n",
    "    x = x_train_cx_small[i]\n",
    "    ax[0,i].imshow(np.real(x).reshape((10,10)),clim=[0,1], cmap= 'viridis')\n",
    "    ax[0,i].set_xticks([])\n",
    "    ax[0,i].set_yticks([])\n",
    "    ax[1,i].imshow(np.imag(x).reshape((10,10)),clim=[0,1], cmap='viridis')\n",
    "    ax[1,i].set_xticks([])\n",
    "    ax[1,i].set_yticks([])\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ea2bf",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "481a9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for training\n",
    "@tf.function\n",
    "def train_step(x, alpha, encoder, decoder, optimizer):\n",
    "    y = decoder(encoder(x))\n",
    "    loss_value = autosetup.loss_MSE(y, x)\n",
    "    grads_and_vars = CBP(x, y, encoder, decoder, autosetup.dLossdaL, autosetup.Jac_modrelu)\n",
    "    new_alpha = adaptive_stepsize(x, y, alpha, encoder, decoder, autosetup.loss_MSE, grads_and_vars)\n",
    "    _ = optimizer.apply_gradients(grads_and_vars, alpha = alpha)\n",
    "    return loss_value, new_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "846f4edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_size 173\n",
      "sample dimension 64\n"
     ]
    }
   ],
   "source": [
    "# parameters (might move somewhere else)\n",
    "EPOCHS = 1000\n",
    "dataset = X_tf[:173] # keep last 10 for testing\n",
    "dataset_size = dataset.shape[0]\n",
    "reshape_dataset = tf.reshape(dataset, (dataset_size,-1))\n",
    "sample_shape = reshape_dataset.shape[-1] # data dimension\n",
    "latent_dim = 30 # reduction of data \n",
    "\n",
    "print(\"dataset_size\", dataset_size)\n",
    "print(\"sample dimension\", sample_shape)\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices(reshape_dataset).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78244d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.batch_op._BatchDataset'>\n",
      "\n",
      "Start of epoch 1\n",
      "Epoch 1 | Loss: 50.43673 | Max grad norm: \n",
      "Time taken: 3.16s\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 | Loss: 14.99907 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch 3 | Loss: 10.62158 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch 4 | Loss: 8.81966 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch 5 | Loss: 7.61742 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch 6 | Loss: 6.85017 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 7\n",
      "Epoch 7 | Loss: 6.28445 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 8\n",
      "Epoch 8 | Loss: 5.71482 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 9\n",
      "Epoch 9 | Loss: 5.32407 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 10\n",
      "Epoch 10 | Loss: 5.07127 | Max grad norm: \n",
      "Time taken: 0.62s\n",
      "\n",
      "Start of epoch 11\n",
      "Epoch 11 | Loss: 4.77699 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 12\n",
      "Epoch 12 | Loss: 4.50939 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 13\n",
      "Epoch 13 | Loss: 4.34879 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 14\n",
      "Epoch 14 | Loss: 4.16495 | Max grad norm: \n",
      "Time taken: 0.58s\n",
      "\n",
      "Start of epoch 15\n",
      "Epoch 15 | Loss: 3.97695 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 16\n",
      "Epoch 16 | Loss: 3.86937 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 17\n",
      "Epoch 17 | Loss: 3.72852 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 18\n",
      "Epoch 18 | Loss: 3.60285 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 19\n",
      "Epoch 19 | Loss: 3.52791 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 20\n",
      "Epoch 20 | Loss: 3.42876 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 21\n",
      "Epoch 21 | Loss: 3.35173 | Max grad norm: \n",
      "Time taken: 0.58s\n",
      "\n",
      "Start of epoch 22\n",
      "Epoch 22 | Loss: 3.27815 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 23\n",
      "Epoch 23 | Loss: 3.18738 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 24\n",
      "Epoch 24 | Loss: 3.13840 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 25\n",
      "Epoch 25 | Loss: 3.06367 | Max grad norm: \n",
      "Time taken: 0.60s\n",
      "\n",
      "Start of epoch 26\n",
      "Epoch 26 | Loss: 3.01415 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 27\n",
      "Epoch 27 | Loss: 2.94729 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 28\n",
      "Epoch 28 | Loss: 2.90558 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 29\n",
      "Epoch 29 | Loss: 2.85303 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 30\n",
      "Epoch 30 | Loss: 2.79579 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 31\n",
      "Epoch 31 | Loss: 2.74905 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 32\n",
      "Epoch 32 | Loss: 2.71802 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 33\n",
      "Epoch 33 | Loss: 2.67140 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 34\n",
      "Epoch 34 | Loss: 2.64048 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 35\n",
      "Epoch 35 | Loss: 2.60328 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 36\n",
      "Epoch 36 | Loss: 2.56809 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 37\n",
      "Epoch 37 | Loss: 2.53115 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 38\n",
      "Epoch 38 | Loss: 2.50434 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 39\n",
      "Epoch 39 | Loss: 2.47426 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 40\n",
      "Epoch 40 | Loss: 2.44323 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 41\n",
      "Epoch 41 | Loss: 2.41825 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 42\n",
      "Epoch 42 | Loss: 2.39012 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 43\n",
      "Epoch 43 | Loss: 2.36189 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 44\n",
      "Epoch 44 | Loss: 2.33598 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 45\n",
      "Epoch 45 | Loss: 2.30797 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 46\n",
      "Epoch 46 | Loss: 2.27416 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 47\n",
      "Epoch 47 | Loss: 2.26211 | Max grad norm: \n",
      "Time taken: 0.58s\n",
      "\n",
      "Start of epoch 48\n",
      "Epoch 48 | Loss: 2.23949 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 49\n",
      "Epoch 49 | Loss: 2.20627 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 50\n",
      "Epoch 50 | Loss: 2.18525 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 51\n",
      "Epoch 51 | Loss: 2.16354 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 52\n",
      "Epoch 52 | Loss: 2.14958 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 53\n",
      "Epoch 53 | Loss: 2.13162 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 54\n",
      "Epoch 54 | Loss: 2.10882 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 55\n",
      "Epoch 55 | Loss: 2.08936 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 56\n",
      "Epoch 56 | Loss: 2.07042 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 57\n",
      "Epoch 57 | Loss: 2.05190 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 58\n",
      "Epoch 58 | Loss: 2.03275 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 59\n",
      "Epoch 59 | Loss: 2.01569 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 60\n",
      "Epoch 60 | Loss: 1.99442 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 61\n",
      "Epoch 61 | Loss: 1.97822 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 62\n",
      "Epoch 62 | Loss: 1.96439 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 63\n",
      "Epoch 63 | Loss: 1.94683 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 64\n",
      "Epoch 64 | Loss: 1.92911 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 65\n",
      "Epoch 65 | Loss: 1.91208 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 66\n",
      "Epoch 66 | Loss: 1.89665 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 67\n",
      "Epoch 67 | Loss: 1.87967 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 68\n",
      "Epoch 68 | Loss: 1.86859 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 69\n",
      "Epoch 69 | Loss: 1.84980 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 70\n",
      "Epoch 70 | Loss: 1.83456 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 71\n",
      "Epoch 71 | Loss: 1.81908 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 72\n",
      "Epoch 72 | Loss: 1.80847 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 73\n",
      "Epoch 73 | Loss: 1.79636 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 74\n",
      "Epoch 74 | Loss: 1.77476 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 75\n",
      "Epoch 75 | Loss: 1.77335 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 76\n",
      "Epoch 76 | Loss: 1.75080 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 77\n",
      "Epoch 77 | Loss: 1.74335 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 78\n",
      "Epoch 78 | Loss: 1.72553 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 79\n",
      "Epoch 79 | Loss: 1.71637 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 80\n",
      "Epoch 80 | Loss: 1.70111 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 81\n",
      "Epoch 81 | Loss: 1.69309 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 82\n",
      "Epoch 82 | Loss: 1.68033 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 83\n",
      "Epoch 83 | Loss: 1.66823 | Max grad norm: \n",
      "Time taken: 0.60s\n",
      "\n",
      "Start of epoch 84\n",
      "Epoch 84 | Loss: 1.65045 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 85\n",
      "Epoch 85 | Loss: 1.64572 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 86\n",
      "Epoch 86 | Loss: 1.63267 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 87\n",
      "Epoch 87 | Loss: 1.62480 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 88\n",
      "Epoch 88 | Loss: 1.60982 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 89\n",
      "Epoch 89 | Loss: 1.59887 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 90\n",
      "Epoch 90 | Loss: 1.58843 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 91\n",
      "Epoch 91 | Loss: 1.57444 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 92\n",
      "Epoch 92 | Loss: 1.56676 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 93\n",
      "Epoch 93 | Loss: 1.55977 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 94\n",
      "Epoch 94 | Loss: 1.54660 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 95\n",
      "Epoch 95 | Loss: 1.53377 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 96\n",
      "Epoch 96 | Loss: 1.52619 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 97\n",
      "Epoch 97 | Loss: 1.51667 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 98\n",
      "Epoch 98 | Loss: 1.50542 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 99\n",
      "Epoch 99 | Loss: 1.49611 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 100\n",
      "Epoch 100 | Loss: 1.48754 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 101\n",
      "Epoch 101 | Loss: 1.47694 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 102\n",
      "Epoch 102 | Loss: 1.46822 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 103\n",
      "Epoch 103 | Loss: 1.45715 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 104\n",
      "Epoch 104 | Loss: 1.44819 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 105\n",
      "Epoch 105 | Loss: 1.44032 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 106\n",
      "Epoch 106 | Loss: 1.43271 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 107\n",
      "Epoch 107 | Loss: 1.41992 | Max grad norm: \n",
      "Time taken: 0.60s\n",
      "\n",
      "Start of epoch 108\n",
      "Epoch 108 | Loss: 1.41306 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 109\n",
      "Epoch 109 | Loss: 1.40751 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 110\n",
      "Epoch 110 | Loss: 1.39817 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 111\n",
      "Epoch 111 | Loss: 1.38679 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 112\n",
      "Epoch 112 | Loss: 1.37731 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 113\n",
      "Epoch 113 | Loss: 1.37614 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 114\n",
      "Epoch 114 | Loss: 1.36656 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 115\n",
      "Epoch 115 | Loss: 1.35500 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 116\n",
      "Epoch 116 | Loss: 1.34807 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 117\n",
      "Epoch 117 | Loss: 1.34444 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 118\n",
      "Epoch 118 | Loss: 1.33272 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 119\n",
      "Epoch 119 | Loss: 1.32507 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 120\n",
      "Epoch 120 | Loss: 1.31794 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 121\n",
      "Epoch 121 | Loss: 1.31344 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 122\n",
      "Epoch 122 | Loss: 1.30383 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 123\n",
      "Epoch 123 | Loss: 1.29806 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 124\n",
      "Epoch 124 | Loss: 1.29042 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 125\n",
      "Epoch 125 | Loss: 1.28396 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 126\n",
      "Epoch 126 | Loss: 1.27608 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 127\n",
      "Epoch 127 | Loss: 1.27034 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 128\n",
      "Epoch 128 | Loss: 1.26028 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 129\n",
      "Epoch 129 | Loss: 1.25858 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 130\n",
      "Epoch 130 | Loss: 1.24834 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 131\n",
      "Epoch 131 | Loss: 1.24510 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 132\n",
      "Epoch 132 | Loss: 1.23807 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 133\n",
      "Epoch 133 | Loss: 1.23149 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 134\n",
      "Epoch 134 | Loss: 1.22405 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 135\n",
      "Epoch 135 | Loss: 1.21877 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 136\n",
      "Epoch 136 | Loss: 1.21462 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 137\n",
      "Epoch 137 | Loss: 1.20771 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 138\n",
      "Epoch 138 | Loss: 1.20124 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 139\n",
      "Epoch 139 | Loss: 1.19641 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 140\n",
      "Epoch 140 | Loss: 1.19180 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 141\n",
      "Epoch 141 | Loss: 1.18534 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 142\n",
      "Epoch 142 | Loss: 1.18025 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 143\n",
      "Epoch 143 | Loss: 1.17496 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 144\n",
      "Epoch 144 | Loss: 1.16907 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 145\n",
      "Epoch 145 | Loss: 1.16468 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 146\n",
      "Epoch 146 | Loss: 1.15752 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 147\n",
      "Epoch 147 | Loss: 1.15290 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 148\n",
      "Epoch 148 | Loss: 1.14572 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 149\n",
      "Epoch 149 | Loss: 1.14185 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 150\n",
      "Epoch 150 | Loss: 1.13679 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 151\n",
      "Epoch 151 | Loss: 1.13194 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 152\n",
      "Epoch 152 | Loss: 1.12661 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 153\n",
      "Epoch 153 | Loss: 1.12205 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 154\n",
      "Epoch 154 | Loss: 1.11731 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 155\n",
      "Epoch 155 | Loss: 1.11185 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 156\n",
      "Epoch 156 | Loss: 1.10923 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 157\n",
      "Epoch 157 | Loss: 1.10385 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 158\n",
      "Epoch 158 | Loss: 1.09882 | Max grad norm: \n",
      "Time taken: 0.58s\n",
      "\n",
      "Start of epoch 159\n",
      "Epoch 159 | Loss: 1.09495 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 160\n",
      "Epoch 160 | Loss: 1.08948 | Max grad norm: \n",
      "Time taken: 0.63s\n",
      "\n",
      "Start of epoch 161\n",
      "Epoch 161 | Loss: 1.08563 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 162\n",
      "Epoch 162 | Loss: 1.08145 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 163\n",
      "Epoch 163 | Loss: 1.07490 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 164\n",
      "Epoch 164 | Loss: 1.07248 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 165\n",
      "Epoch 165 | Loss: 1.06714 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 166\n",
      "Epoch 166 | Loss: 1.06444 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 167\n",
      "Epoch 167 | Loss: 1.05976 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 168\n",
      "Epoch 168 | Loss: 1.05536 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 169\n",
      "Epoch 169 | Loss: 1.04914 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 170\n",
      "Epoch 170 | Loss: 1.04690 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 171\n",
      "Epoch 171 | Loss: 1.04321 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 172\n",
      "Epoch 172 | Loss: 1.03988 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 173\n",
      "Epoch 173 | Loss: 1.03492 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 174\n",
      "Epoch 174 | Loss: 1.03044 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 175\n",
      "Epoch 175 | Loss: 1.02847 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 176\n",
      "Epoch 176 | Loss: 1.02292 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 177\n",
      "Epoch 177 | Loss: 1.01861 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 178\n",
      "Epoch 178 | Loss: 1.01663 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 179\n",
      "Epoch 179 | Loss: 1.01116 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 180\n",
      "Epoch 180 | Loss: 1.00708 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 181\n",
      "Epoch 181 | Loss: 1.00490 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 182\n",
      "Epoch 182 | Loss: 1.00207 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 183\n",
      "Epoch 183 | Loss: 0.99729 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 184\n",
      "Epoch 184 | Loss: 0.99419 | Max grad norm: \n",
      "Time taken: 0.60s\n",
      "\n",
      "Start of epoch 185\n",
      "Epoch 185 | Loss: 0.99087 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 186\n",
      "Epoch 186 | Loss: 0.98873 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 187\n",
      "Epoch 187 | Loss: 0.98452 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 188\n",
      "Epoch 188 | Loss: 0.97941 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 189\n",
      "Epoch 189 | Loss: 0.97536 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 190\n",
      "Epoch 190 | Loss: 0.97374 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 191\n",
      "Epoch 191 | Loss: 0.96784 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 192\n",
      "Epoch 192 | Loss: 0.96780 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 193\n",
      "Epoch 193 | Loss: 0.96237 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 194\n",
      "Epoch 194 | Loss: 0.95832 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 195\n",
      "Epoch 195 | Loss: 0.95791 | Max grad norm: \n",
      "Time taken: 0.79s\n",
      "\n",
      "Start of epoch 196\n",
      "Epoch 196 | Loss: 0.95259 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 197\n",
      "Epoch 197 | Loss: 0.95084 | Max grad norm: \n",
      "Time taken: 0.82s\n",
      "\n",
      "Start of epoch 198\n",
      "Epoch 198 | Loss: 0.94800 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 199\n",
      "Epoch 199 | Loss: 0.94345 | Max grad norm: \n",
      "Time taken: 0.78s\n",
      "\n",
      "Start of epoch 200\n",
      "Epoch 200 | Loss: 0.94076 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 201\n",
      "Epoch 201 | Loss: 0.93670 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 202\n",
      "Epoch 202 | Loss: 0.93419 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 203\n",
      "Epoch 203 | Loss: 0.93195 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 204\n",
      "Epoch 204 | Loss: 0.92955 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 205\n",
      "Epoch 205 | Loss: 0.92521 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 206\n",
      "Epoch 206 | Loss: 0.92226 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 207\n",
      "Epoch 207 | Loss: 0.91816 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 208\n",
      "Epoch 208 | Loss: 0.91561 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 209\n",
      "Epoch 209 | Loss: 0.91608 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 210\n",
      "Epoch 210 | Loss: 0.91153 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 211\n",
      "Epoch 211 | Loss: 0.90773 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 212\n",
      "Epoch 212 | Loss: 0.90582 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 213\n",
      "Epoch 213 | Loss: 0.90253 | Max grad norm: \n",
      "Time taken: 0.80s\n",
      "\n",
      "Start of epoch 214\n",
      "Epoch 214 | Loss: 0.90005 | Max grad norm: \n",
      "Time taken: 0.82s\n",
      "\n",
      "Start of epoch 215\n",
      "Epoch 215 | Loss: 0.89733 | Max grad norm: \n",
      "Time taken: 0.90s\n",
      "\n",
      "Start of epoch 216\n",
      "Epoch 216 | Loss: 0.89462 | Max grad norm: \n",
      "Time taken: 0.83s\n",
      "\n",
      "Start of epoch 217\n",
      "Epoch 217 | Loss: 0.89165 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 218\n",
      "Epoch 218 | Loss: 0.88957 | Max grad norm: \n",
      "Time taken: 0.83s\n",
      "\n",
      "Start of epoch 219\n",
      "Epoch 219 | Loss: 0.88698 | Max grad norm: \n",
      "Time taken: 0.84s\n",
      "\n",
      "Start of epoch 220\n",
      "Epoch 220 | Loss: 0.88496 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 221\n",
      "Epoch 221 | Loss: 0.88063 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 222\n",
      "Epoch 222 | Loss: 0.87894 | Max grad norm: \n",
      "Time taken: 0.63s\n",
      "\n",
      "Start of epoch 223\n",
      "Epoch 223 | Loss: 0.87632 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 224\n",
      "Epoch 224 | Loss: 0.87224 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 225\n",
      "Epoch 225 | Loss: 0.87008 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 226\n",
      "Epoch 226 | Loss: 0.86791 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 227\n",
      "Epoch 227 | Loss: 0.86624 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 228\n",
      "Epoch 228 | Loss: 0.86267 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 229\n",
      "Epoch 229 | Loss: 0.85972 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 230\n",
      "Epoch 230 | Loss: 0.85655 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 231\n",
      "Epoch 231 | Loss: 0.85585 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 232\n",
      "Epoch 232 | Loss: 0.85327 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 233\n",
      "Epoch 233 | Loss: 0.84943 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 234\n",
      "Epoch 234 | Loss: 0.84959 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 235\n",
      "Epoch 235 | Loss: 0.84579 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 236\n",
      "Epoch 236 | Loss: 0.84220 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 237\n",
      "Epoch 237 | Loss: 0.84105 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 238\n",
      "Epoch 238 | Loss: 0.83967 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 239\n",
      "Epoch 239 | Loss: 0.83700 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 240\n",
      "Epoch 240 | Loss: 0.83383 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 241\n",
      "Epoch 241 | Loss: 0.83233 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 242\n",
      "Epoch 242 | Loss: 0.82963 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 243\n",
      "Epoch 243 | Loss: 0.82618 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 244\n",
      "Epoch 244 | Loss: 0.82515 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 245\n",
      "Epoch 245 | Loss: 0.82182 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 246\n",
      "Epoch 246 | Loss: 0.82255 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 247\n",
      "Epoch 247 | Loss: 0.81769 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 248\n",
      "Epoch 248 | Loss: 0.81785 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 249\n",
      "Epoch 249 | Loss: 0.81330 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 250\n",
      "Epoch 250 | Loss: 0.81156 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 251\n",
      "Epoch 251 | Loss: 0.80998 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 252\n",
      "Epoch 252 | Loss: 0.80782 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 253\n",
      "Epoch 253 | Loss: 0.80559 | Max grad norm: \n",
      "Time taken: 0.62s\n",
      "\n",
      "Start of epoch 254\n",
      "Epoch 254 | Loss: 0.80354 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 255\n",
      "Epoch 255 | Loss: 0.80131 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 256\n",
      "Epoch 256 | Loss: 0.80035 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 257\n",
      "Epoch 257 | Loss: 0.79705 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 258\n",
      "Epoch 258 | Loss: 0.79575 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 259\n",
      "Epoch 259 | Loss: 0.79361 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 260\n",
      "Epoch 260 | Loss: 0.79192 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 261\n",
      "Epoch 261 | Loss: 0.78908 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 262\n",
      "Epoch 262 | Loss: 0.78769 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 263\n",
      "Epoch 263 | Loss: 0.78546 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 264\n",
      "Epoch 264 | Loss: 0.78428 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 265\n",
      "Epoch 265 | Loss: 0.78198 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 266\n",
      "Epoch 266 | Loss: 0.77898 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 267\n",
      "Epoch 267 | Loss: 0.77796 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 268\n",
      "Epoch 268 | Loss: 0.77676 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 269\n",
      "Epoch 269 | Loss: 0.77415 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 270\n",
      "Epoch 270 | Loss: 0.77127 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 271\n",
      "Epoch 271 | Loss: 0.76963 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 272\n",
      "Epoch 272 | Loss: 0.76761 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 273\n",
      "Epoch 273 | Loss: 0.76638 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 274\n",
      "Epoch 274 | Loss: 0.76580 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 275\n",
      "Epoch 275 | Loss: 0.76221 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 276\n",
      "Epoch 276 | Loss: 0.76157 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 277\n",
      "Epoch 277 | Loss: 0.75902 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 278\n",
      "Epoch 278 | Loss: 0.75573 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 279\n",
      "Epoch 279 | Loss: 0.75410 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 280\n",
      "Epoch 280 | Loss: 0.75512 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 281\n",
      "Epoch 281 | Loss: 0.75089 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 282\n",
      "Epoch 282 | Loss: 0.74945 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 283\n",
      "Epoch 283 | Loss: 0.74825 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 284\n",
      "Epoch 284 | Loss: 0.74565 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 285\n",
      "Epoch 285 | Loss: 0.74353 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 286\n",
      "Epoch 286 | Loss: 0.74322 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 287\n",
      "Epoch 287 | Loss: 0.73895 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 288\n",
      "Epoch 288 | Loss: 0.74005 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 289\n",
      "Epoch 289 | Loss: 0.73732 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 290\n",
      "Epoch 290 | Loss: 0.73572 | Max grad norm: \n",
      "Time taken: 0.47s\n",
      "\n",
      "Start of epoch 291\n",
      "Epoch 291 | Loss: 0.73405 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 292\n",
      "Epoch 292 | Loss: 0.73199 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 293\n",
      "Epoch 293 | Loss: 0.72890 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 294\n",
      "Epoch 294 | Loss: 0.72910 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 295\n",
      "Epoch 295 | Loss: 0.72743 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 296\n",
      "Epoch 296 | Loss: 0.72559 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 297\n",
      "Epoch 297 | Loss: 0.72387 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 298\n",
      "Epoch 298 | Loss: 0.72155 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 299\n",
      "Epoch 299 | Loss: 0.71999 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 300\n",
      "Epoch 300 | Loss: 0.71819 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 301\n",
      "Epoch 301 | Loss: 0.71769 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 302\n",
      "Epoch 302 | Loss: 0.71519 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 303\n",
      "Epoch 303 | Loss: 0.71403 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 304\n",
      "Epoch 304 | Loss: 0.71191 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 305\n",
      "Epoch 305 | Loss: 0.71104 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 306\n",
      "Epoch 306 | Loss: 0.71040 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 307\n",
      "Epoch 307 | Loss: 0.70746 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 308\n",
      "Epoch 308 | Loss: 0.70623 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 309\n",
      "Epoch 309 | Loss: 0.70498 | Max grad norm: \n",
      "Time taken: 0.58s\n",
      "\n",
      "Start of epoch 310\n",
      "Epoch 310 | Loss: 0.70442 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 311\n",
      "Epoch 311 | Loss: 0.70144 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 312\n",
      "Epoch 312 | Loss: 0.70062 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 313\n",
      "Epoch 313 | Loss: 0.69904 | Max grad norm: \n",
      "Time taken: 0.60s\n",
      "\n",
      "Start of epoch 314\n",
      "Epoch 314 | Loss: 0.69797 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 315\n",
      "Epoch 315 | Loss: 0.69665 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 316\n",
      "Epoch 316 | Loss: 0.69437 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 317\n",
      "Epoch 317 | Loss: 0.69382 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 318\n",
      "Epoch 318 | Loss: 0.69190 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 319\n",
      "Epoch 319 | Loss: 0.69098 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 320\n",
      "Epoch 320 | Loss: 0.68834 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 321\n",
      "Epoch 321 | Loss: 0.68800 | Max grad norm: \n",
      "Time taken: 0.85s\n",
      "\n",
      "Start of epoch 322\n",
      "Epoch 322 | Loss: 0.68505 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 323\n",
      "Epoch 323 | Loss: 0.68521 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 324\n",
      "Epoch 324 | Loss: 0.68289 | Max grad norm: \n",
      "Time taken: 0.78s\n",
      "\n",
      "Start of epoch 325\n",
      "Epoch 325 | Loss: 0.68060 | Max grad norm: \n",
      "Time taken: 0.84s\n",
      "\n",
      "Start of epoch 326\n",
      "Epoch 326 | Loss: 0.68091 | Max grad norm: \n",
      "Time taken: 0.81s\n",
      "\n",
      "Start of epoch 327\n",
      "Epoch 327 | Loss: 0.67944 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 328\n",
      "Epoch 328 | Loss: 0.67814 | Max grad norm: \n",
      "Time taken: 0.60s\n",
      "\n",
      "Start of epoch 329\n",
      "Epoch 329 | Loss: 0.67566 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 330\n",
      "Epoch 330 | Loss: 0.67563 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 331\n",
      "Epoch 331 | Loss: 0.67384 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 332\n",
      "Epoch 332 | Loss: 0.67165 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 333\n",
      "Epoch 333 | Loss: 0.67043 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 334\n",
      "Epoch 334 | Loss: 0.66881 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 335\n",
      "Epoch 335 | Loss: 0.66925 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 336\n",
      "Epoch 336 | Loss: 0.66686 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 337\n",
      "Epoch 337 | Loss: 0.66335 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 338\n",
      "Epoch 338 | Loss: 0.66491 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 339\n",
      "Epoch 339 | Loss: 0.66223 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 340\n",
      "Epoch 340 | Loss: 0.66163 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 341\n",
      "Epoch 341 | Loss: 0.65912 | Max grad norm: \n",
      "Time taken: 0.62s\n",
      "\n",
      "Start of epoch 342\n",
      "Epoch 342 | Loss: 0.65777 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 343\n",
      "Epoch 343 | Loss: 0.65869 | Max grad norm: \n",
      "Time taken: 0.58s\n",
      "\n",
      "Start of epoch 344\n",
      "Epoch 344 | Loss: 0.65472 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 345\n",
      "Epoch 345 | Loss: 0.65544 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 346\n",
      "Epoch 346 | Loss: 0.65438 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 347\n",
      "Epoch 347 | Loss: 0.65123 | Max grad norm: \n",
      "Time taken: 0.58s\n",
      "\n",
      "Start of epoch 348\n",
      "Epoch 348 | Loss: 0.65143 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 349\n",
      "Epoch 349 | Loss: 0.65036 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 350\n",
      "Epoch 350 | Loss: 0.64927 | Max grad norm: \n",
      "Time taken: 0.78s\n",
      "\n",
      "Start of epoch 351\n",
      "Epoch 351 | Loss: 0.64730 | Max grad norm: \n",
      "Time taken: 0.85s\n",
      "\n",
      "Start of epoch 352\n",
      "Epoch 352 | Loss: 0.64610 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 353\n",
      "Epoch 353 | Loss: 0.64552 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 354\n",
      "Epoch 354 | Loss: 0.64386 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 355\n",
      "Epoch 355 | Loss: 0.64198 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 356\n",
      "Epoch 356 | Loss: 0.64153 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 357\n",
      "Epoch 357 | Loss: 0.64122 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 358\n",
      "Epoch 358 | Loss: 0.63982 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 359\n",
      "Epoch 359 | Loss: 0.63785 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 360\n",
      "Epoch 360 | Loss: 0.63692 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 361\n",
      "Epoch 361 | Loss: 0.63602 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 362\n",
      "Epoch 362 | Loss: 0.63381 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 363\n",
      "Epoch 363 | Loss: 0.63277 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 364\n",
      "Epoch 364 | Loss: 0.63235 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 365\n",
      "Epoch 365 | Loss: 0.63074 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 366\n",
      "Epoch 366 | Loss: 0.62940 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 367\n",
      "Epoch 367 | Loss: 0.62836 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 368\n",
      "Epoch 368 | Loss: 0.62788 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 369\n",
      "Epoch 369 | Loss: 0.62705 | Max grad norm: \n",
      "Time taken: 0.78s\n",
      "\n",
      "Start of epoch 370\n",
      "Epoch 370 | Loss: 0.62519 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 371\n",
      "Epoch 371 | Loss: 0.62502 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 372\n",
      "Epoch 372 | Loss: 0.62265 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 373\n",
      "Epoch 373 | Loss: 0.62193 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 374\n",
      "Epoch 374 | Loss: 0.62120 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 375\n",
      "Epoch 375 | Loss: 0.62017 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 376\n",
      "Epoch 376 | Loss: 0.61811 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 377\n",
      "Epoch 377 | Loss: 0.61756 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 378\n",
      "Epoch 378 | Loss: 0.61636 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 379\n",
      "Epoch 379 | Loss: 0.61550 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 380\n",
      "Epoch 380 | Loss: 0.61514 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 381\n",
      "Epoch 381 | Loss: 0.61337 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 382\n",
      "Epoch 382 | Loss: 0.61228 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 383\n",
      "Epoch 383 | Loss: 0.61083 | Max grad norm: \n",
      "Time taken: 0.79s\n",
      "\n",
      "Start of epoch 384\n",
      "Epoch 384 | Loss: 0.61077 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 385\n",
      "Epoch 385 | Loss: 0.60811 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 386\n",
      "Epoch 386 | Loss: 0.60770 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 387\n",
      "Epoch 387 | Loss: 0.60715 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 388\n",
      "Epoch 388 | Loss: 0.60628 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 389\n",
      "Epoch 389 | Loss: 0.60484 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 390\n",
      "Epoch 390 | Loss: 0.60471 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 391\n",
      "Epoch 391 | Loss: 0.60303 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 392\n",
      "Epoch 392 | Loss: 0.60118 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 393\n",
      "Epoch 393 | Loss: 0.60089 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 394\n",
      "Epoch 394 | Loss: 0.60021 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 395\n",
      "Epoch 395 | Loss: 0.59905 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 396\n",
      "Epoch 396 | Loss: 0.59803 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 397\n",
      "Epoch 397 | Loss: 0.59728 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 398\n",
      "Epoch 398 | Loss: 0.59548 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 399\n",
      "Epoch 399 | Loss: 0.59483 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 400\n",
      "Epoch 400 | Loss: 0.59390 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 401\n",
      "Epoch 401 | Loss: 0.59307 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 402\n",
      "Epoch 402 | Loss: 0.59195 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 403\n",
      "Epoch 403 | Loss: 0.59042 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 404\n",
      "Epoch 404 | Loss: 0.59014 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 405\n",
      "Epoch 405 | Loss: 0.58898 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 406\n",
      "Epoch 406 | Loss: 0.58849 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 407\n",
      "Epoch 407 | Loss: 0.58738 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 408\n",
      "Epoch 408 | Loss: 0.58699 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 409\n",
      "Epoch 409 | Loss: 0.58494 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 410\n",
      "Epoch 410 | Loss: 0.58390 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 411\n",
      "Epoch 411 | Loss: 0.58258 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 412\n",
      "Epoch 412 | Loss: 0.58168 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 413\n",
      "Epoch 413 | Loss: 0.58172 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 414\n",
      "Epoch 414 | Loss: 0.58058 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 415\n",
      "Epoch 415 | Loss: 0.57928 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 416\n",
      "Epoch 416 | Loss: 0.57811 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 417\n",
      "Epoch 417 | Loss: 0.57796 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 418\n",
      "Epoch 418 | Loss: 0.57670 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 419\n",
      "Epoch 419 | Loss: 0.57494 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 420\n",
      "Epoch 420 | Loss: 0.57406 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 421\n",
      "Epoch 421 | Loss: 0.57420 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 422\n",
      "Epoch 422 | Loss: 0.57213 | Max grad norm: \n",
      "Time taken: 0.79s\n",
      "\n",
      "Start of epoch 423\n",
      "Epoch 423 | Loss: 0.57217 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 424\n",
      "Epoch 424 | Loss: 0.57089 | Max grad norm: \n",
      "Time taken: 0.85s\n",
      "\n",
      "Start of epoch 425\n",
      "Epoch 425 | Loss: 0.57016 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 426\n",
      "Epoch 426 | Loss: 0.56911 | Max grad norm: \n",
      "Time taken: 0.60s\n",
      "\n",
      "Start of epoch 427\n",
      "Epoch 427 | Loss: 0.56866 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 428\n",
      "Epoch 428 | Loss: 0.56842 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 429\n",
      "Epoch 429 | Loss: 0.56633 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 430\n",
      "Epoch 430 | Loss: 0.56557 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 431\n",
      "Epoch 431 | Loss: 0.56413 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 432\n",
      "Epoch 432 | Loss: 0.56396 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 433\n",
      "Epoch 433 | Loss: 0.56330 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 434\n",
      "Epoch 434 | Loss: 0.56269 | Max grad norm: \n",
      "Time taken: 0.63s\n",
      "\n",
      "Start of epoch 435\n",
      "Epoch 435 | Loss: 0.56187 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 436\n",
      "Epoch 436 | Loss: 0.56015 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 437\n",
      "Epoch 437 | Loss: 0.55969 | Max grad norm: \n",
      "Time taken: 0.80s\n",
      "\n",
      "Start of epoch 438\n",
      "Epoch 438 | Loss: 0.56005 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 439\n",
      "Epoch 439 | Loss: 0.55746 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 440\n",
      "Epoch 440 | Loss: 0.55670 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 441\n",
      "Epoch 441 | Loss: 0.55621 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 442\n",
      "Epoch 442 | Loss: 0.55585 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 443\n",
      "Epoch 443 | Loss: 0.55534 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 444\n",
      "Epoch 444 | Loss: 0.55427 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 445\n",
      "Epoch 445 | Loss: 0.55319 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 446\n",
      "Epoch 446 | Loss: 0.55192 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 447\n",
      "Epoch 447 | Loss: 0.55102 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 448\n",
      "Epoch 448 | Loss: 0.55073 | Max grad norm: \n",
      "Time taken: 0.79s\n",
      "\n",
      "Start of epoch 449\n",
      "Epoch 449 | Loss: 0.54984 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 450\n",
      "Epoch 450 | Loss: 0.54953 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 451\n",
      "Epoch 451 | Loss: 0.54869 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 452\n",
      "Epoch 452 | Loss: 0.54764 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 453\n",
      "Epoch 453 | Loss: 0.54604 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 454\n",
      "Epoch 454 | Loss: 0.54632 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 455\n",
      "Epoch 455 | Loss: 0.54516 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 456\n",
      "Epoch 456 | Loss: 0.54391 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 457\n",
      "Epoch 457 | Loss: 0.54279 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 458\n",
      "Epoch 458 | Loss: 0.54222 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 459\n",
      "Epoch 459 | Loss: 0.54251 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 460\n",
      "Epoch 460 | Loss: 0.54128 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 461\n",
      "Epoch 461 | Loss: 0.54046 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 462\n",
      "Epoch 462 | Loss: 0.53971 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 463\n",
      "Epoch 463 | Loss: 0.53841 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 464\n",
      "Epoch 464 | Loss: 0.53721 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 465\n",
      "Epoch 465 | Loss: 0.53685 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 466\n",
      "Epoch 466 | Loss: 0.53594 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 467\n",
      "Epoch 467 | Loss: 0.53498 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 468\n",
      "Epoch 468 | Loss: 0.53470 | Max grad norm: \n",
      "Time taken: 0.63s\n",
      "\n",
      "Start of epoch 469\n",
      "Epoch 469 | Loss: 0.53392 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 470\n",
      "Epoch 470 | Loss: 0.53339 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 471\n",
      "Epoch 471 | Loss: 0.53198 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 472\n",
      "Epoch 472 | Loss: 0.53102 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 473\n",
      "Epoch 473 | Loss: 0.53081 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 474\n",
      "Epoch 474 | Loss: 0.53026 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 475\n",
      "Epoch 475 | Loss: 0.52926 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 476\n",
      "Epoch 476 | Loss: 0.52905 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 477\n",
      "Epoch 477 | Loss: 0.52723 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 478\n",
      "Epoch 478 | Loss: 0.52668 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 479\n",
      "Epoch 479 | Loss: 0.52597 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 480\n",
      "Epoch 480 | Loss: 0.52527 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 481\n",
      "Epoch 481 | Loss: 0.52403 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 482\n",
      "Epoch 482 | Loss: 0.52474 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 483\n",
      "Epoch 483 | Loss: 0.52257 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 484\n",
      "Epoch 484 | Loss: 0.52260 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 485\n",
      "Epoch 485 | Loss: 0.52135 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 486\n",
      "Epoch 486 | Loss: 0.52144 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 487\n",
      "Epoch 487 | Loss: 0.52045 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 488\n",
      "Epoch 488 | Loss: 0.51906 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 489\n",
      "Epoch 489 | Loss: 0.51909 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 490\n",
      "Epoch 490 | Loss: 0.51809 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 491\n",
      "Epoch 491 | Loss: 0.51723 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 492\n",
      "Epoch 492 | Loss: 0.51663 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 493\n",
      "Epoch 493 | Loss: 0.51658 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 494\n",
      "Epoch 494 | Loss: 0.51521 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 495\n",
      "Epoch 495 | Loss: 0.51476 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 496\n",
      "Epoch 496 | Loss: 0.51352 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 497\n",
      "Epoch 497 | Loss: 0.51317 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 498\n",
      "Epoch 498 | Loss: 0.51199 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 499\n",
      "Epoch 499 | Loss: 0.51216 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 500\n",
      "Epoch 500 | Loss: 0.51193 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 501\n",
      "Epoch 501 | Loss: 0.51031 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 502\n",
      "Epoch 502 | Loss: 0.50967 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 503\n",
      "Epoch 503 | Loss: 0.50956 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 504\n",
      "Epoch 504 | Loss: 0.50779 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 505\n",
      "Epoch 505 | Loss: 0.50754 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 506\n",
      "Epoch 506 | Loss: 0.50619 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 507\n",
      "Epoch 507 | Loss: 0.50646 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 508\n",
      "Epoch 508 | Loss: 0.50439 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 509\n",
      "Epoch 509 | Loss: 0.50469 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 510\n",
      "Epoch 510 | Loss: 0.50438 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 511\n",
      "Epoch 511 | Loss: 0.50418 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 512\n",
      "Epoch 512 | Loss: 0.50272 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 513\n",
      "Epoch 513 | Loss: 0.50184 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 514\n",
      "Epoch 514 | Loss: 0.50112 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 515\n",
      "Epoch 515 | Loss: 0.50066 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 516\n",
      "Epoch 516 | Loss: 0.50004 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 517\n",
      "Epoch 517 | Loss: 0.49983 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 518\n",
      "Epoch 518 | Loss: 0.49872 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 519\n",
      "Epoch 519 | Loss: 0.49775 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 520\n",
      "Epoch 520 | Loss: 0.49728 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 521\n",
      "Epoch 521 | Loss: 0.49693 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 522\n",
      "Epoch 522 | Loss: 0.49611 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 523\n",
      "Epoch 523 | Loss: 0.49610 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 524\n",
      "Epoch 524 | Loss: 0.49501 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 525\n",
      "Epoch 525 | Loss: 0.49460 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 526\n",
      "Epoch 526 | Loss: 0.49396 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 527\n",
      "Epoch 527 | Loss: 0.49285 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 528\n",
      "Epoch 528 | Loss: 0.49266 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 529\n",
      "Epoch 529 | Loss: 0.49177 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 530\n",
      "Epoch 530 | Loss: 0.49028 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 531\n",
      "Epoch 531 | Loss: 0.49090 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 532\n",
      "Epoch 532 | Loss: 0.48966 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 533\n",
      "Epoch 533 | Loss: 0.48987 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 534\n",
      "Epoch 534 | Loss: 0.48860 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 535\n",
      "Epoch 535 | Loss: 0.48811 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 536\n",
      "Epoch 536 | Loss: 0.48748 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 537\n",
      "Epoch 537 | Loss: 0.48622 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 538\n",
      "Epoch 538 | Loss: 0.48605 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 539\n",
      "Epoch 539 | Loss: 0.48573 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 540\n",
      "Epoch 540 | Loss: 0.48428 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 541\n",
      "Epoch 541 | Loss: 0.48504 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 542\n",
      "Epoch 542 | Loss: 0.48393 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 543\n",
      "Epoch 543 | Loss: 0.48299 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 544\n",
      "Epoch 544 | Loss: 0.48216 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 545\n",
      "Epoch 545 | Loss: 0.48193 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 546\n",
      "Epoch 546 | Loss: 0.48124 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 547\n",
      "Epoch 547 | Loss: 0.48068 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 548\n",
      "Epoch 548 | Loss: 0.48010 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 549\n",
      "Epoch 549 | Loss: 0.47970 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 550\n",
      "Epoch 550 | Loss: 0.47905 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 551\n",
      "Epoch 551 | Loss: 0.47781 | Max grad norm: \n",
      "Time taken: 0.50s\n",
      "\n",
      "Start of epoch 552\n",
      "Epoch 552 | Loss: 0.47762 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 553\n",
      "Epoch 553 | Loss: 0.47677 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 554\n",
      "Epoch 554 | Loss: 0.47628 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 555\n",
      "Epoch 555 | Loss: 0.47523 | Max grad norm: \n",
      "Time taken: 0.53s\n",
      "\n",
      "Start of epoch 556\n",
      "Epoch 556 | Loss: 0.47517 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 557\n",
      "Epoch 557 | Loss: 0.47431 | Max grad norm: \n",
      "Time taken: 0.51s\n",
      "\n",
      "Start of epoch 558\n",
      "Epoch 558 | Loss: 0.47424 | Max grad norm: \n",
      "Time taken: 0.58s\n",
      "\n",
      "Start of epoch 559\n",
      "Epoch 559 | Loss: 0.47325 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 560\n",
      "Epoch 560 | Loss: 0.47339 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 561\n",
      "Epoch 561 | Loss: 0.47283 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 562\n",
      "Epoch 562 | Loss: 0.47184 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 563\n",
      "Epoch 563 | Loss: 0.47073 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 564\n",
      "Epoch 564 | Loss: 0.47080 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 565\n",
      "Epoch 565 | Loss: 0.46999 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 566\n",
      "Epoch 566 | Loss: 0.46940 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 567\n",
      "Epoch 567 | Loss: 0.46901 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 568\n",
      "Epoch 568 | Loss: 0.46846 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 569\n",
      "Epoch 569 | Loss: 0.46792 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 570\n",
      "Epoch 570 | Loss: 0.46700 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 571\n",
      "Epoch 571 | Loss: 0.46718 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 572\n",
      "Epoch 572 | Loss: 0.46617 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 573\n",
      "Epoch 573 | Loss: 0.46552 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 574\n",
      "Epoch 574 | Loss: 0.46428 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 575\n",
      "Epoch 575 | Loss: 0.46466 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 576\n",
      "Epoch 576 | Loss: 0.46427 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 577\n",
      "Epoch 577 | Loss: 0.46317 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 578\n",
      "Epoch 578 | Loss: 0.46269 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 579\n",
      "Epoch 579 | Loss: 0.46222 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 580\n",
      "Epoch 580 | Loss: 0.46125 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 581\n",
      "Epoch 581 | Loss: 0.46129 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 582\n",
      "Epoch 582 | Loss: 0.46050 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 583\n",
      "Epoch 583 | Loss: 0.45991 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 584\n",
      "Epoch 584 | Loss: 0.45987 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 585\n",
      "Epoch 585 | Loss: 0.45872 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 586\n",
      "Epoch 586 | Loss: 0.45795 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 587\n",
      "Epoch 587 | Loss: 0.45773 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 588\n",
      "Epoch 588 | Loss: 0.45681 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 589\n",
      "Epoch 589 | Loss: 0.45689 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 590\n",
      "Epoch 590 | Loss: 0.45671 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 591\n",
      "Epoch 591 | Loss: 0.45558 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 592\n",
      "Epoch 592 | Loss: 0.45508 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 593\n",
      "Epoch 593 | Loss: 0.45454 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 594\n",
      "Epoch 594 | Loss: 0.45420 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 595\n",
      "Epoch 595 | Loss: 0.45317 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 596\n",
      "Epoch 596 | Loss: 0.45322 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 597\n",
      "Epoch 597 | Loss: 0.45289 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 598\n",
      "Epoch 598 | Loss: 0.45185 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 599\n",
      "Epoch 599 | Loss: 0.45142 | Max grad norm: \n",
      "Time taken: 0.63s\n",
      "\n",
      "Start of epoch 600\n",
      "Epoch 600 | Loss: 0.45114 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 601\n",
      "Epoch 601 | Loss: 0.45023 | Max grad norm: \n",
      "Time taken: 0.80s\n",
      "\n",
      "Start of epoch 602\n",
      "Epoch 602 | Loss: 0.44978 | Max grad norm: \n",
      "Time taken: 0.80s\n",
      "\n",
      "Start of epoch 603\n",
      "Epoch 603 | Loss: 0.44972 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 604\n",
      "Epoch 604 | Loss: 0.44906 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 605\n",
      "Epoch 605 | Loss: 0.44870 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 606\n",
      "Epoch 606 | Loss: 0.44862 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 607\n",
      "Epoch 607 | Loss: 0.44733 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 608\n",
      "Epoch 608 | Loss: 0.44705 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 609\n",
      "Epoch 609 | Loss: 0.44625 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 610\n",
      "Epoch 610 | Loss: 0.44609 | Max grad norm: \n",
      "Time taken: 0.81s\n",
      "\n",
      "Start of epoch 611\n",
      "Epoch 611 | Loss: 0.44540 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 612\n",
      "Epoch 612 | Loss: 0.44481 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 613\n",
      "Epoch 613 | Loss: 0.44468 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 614\n",
      "Epoch 614 | Loss: 0.44446 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 615\n",
      "Epoch 615 | Loss: 0.44340 | Max grad norm: \n",
      "Time taken: 0.57s\n",
      "\n",
      "Start of epoch 616\n",
      "Epoch 616 | Loss: 0.44295 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 617\n",
      "Epoch 617 | Loss: 0.44257 | Max grad norm: \n",
      "Time taken: 0.48s\n",
      "\n",
      "Start of epoch 618\n",
      "Epoch 618 | Loss: 0.44178 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 619\n",
      "Epoch 619 | Loss: 0.44198 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 620\n",
      "Epoch 620 | Loss: 0.44095 | Max grad norm: \n",
      "Time taken: 0.46s\n",
      "\n",
      "Start of epoch 621\n",
      "Epoch 621 | Loss: 0.44092 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 622\n",
      "Epoch 622 | Loss: 0.44063 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 623\n",
      "Epoch 623 | Loss: 0.43944 | Max grad norm: \n",
      "Time taken: 0.54s\n",
      "\n",
      "Start of epoch 624\n",
      "Epoch 624 | Loss: 0.43873 | Max grad norm: \n",
      "Time taken: 0.62s\n",
      "\n",
      "Start of epoch 625\n",
      "Epoch 625 | Loss: 0.43876 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 626\n",
      "Epoch 626 | Loss: 0.43805 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 627\n",
      "Epoch 627 | Loss: 0.43791 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 628\n",
      "Epoch 628 | Loss: 0.43694 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 629\n",
      "Epoch 629 | Loss: 0.43665 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 630\n",
      "Epoch 630 | Loss: 0.43644 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 631\n",
      "Epoch 631 | Loss: 0.43575 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 632\n",
      "Epoch 632 | Loss: 0.43581 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 633\n",
      "Epoch 633 | Loss: 0.43464 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 634\n",
      "Epoch 634 | Loss: 0.43391 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 635\n",
      "Epoch 635 | Loss: 0.43312 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 636\n",
      "Epoch 636 | Loss: 0.43323 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 637\n",
      "Epoch 637 | Loss: 0.43296 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 638\n",
      "Epoch 638 | Loss: 0.43254 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 639\n",
      "Epoch 639 | Loss: 0.43198 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 640\n",
      "Epoch 640 | Loss: 0.43103 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 641\n",
      "Epoch 641 | Loss: 0.43014 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 642\n",
      "Epoch 642 | Loss: 0.43056 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 643\n",
      "Epoch 643 | Loss: 0.42965 | Max grad norm: \n",
      "Time taken: 0.58s\n",
      "\n",
      "Start of epoch 644\n",
      "Epoch 644 | Loss: 0.43004 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 645\n",
      "Epoch 645 | Loss: 0.42869 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 646\n",
      "Epoch 646 | Loss: 0.42874 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 647\n",
      "Epoch 647 | Loss: 0.42806 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 648\n",
      "Epoch 648 | Loss: 0.42825 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 649\n",
      "Epoch 649 | Loss: 0.42723 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 650\n",
      "Epoch 650 | Loss: 0.42643 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 651\n",
      "Epoch 651 | Loss: 0.42579 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 652\n",
      "Epoch 652 | Loss: 0.42587 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 653\n",
      "Epoch 653 | Loss: 0.42519 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 654\n",
      "Epoch 654 | Loss: 0.42477 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 655\n",
      "Epoch 655 | Loss: 0.42410 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 656\n",
      "Epoch 656 | Loss: 0.42466 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 657\n",
      "Epoch 657 | Loss: 0.42340 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 658\n",
      "Epoch 658 | Loss: 0.42289 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 659\n",
      "Epoch 659 | Loss: 0.42323 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 660\n",
      "Epoch 660 | Loss: 0.42224 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 661\n",
      "Epoch 661 | Loss: 0.42235 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 662\n",
      "Epoch 662 | Loss: 0.42141 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 663\n",
      "Epoch 663 | Loss: 0.42120 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 664\n",
      "Epoch 664 | Loss: 0.42020 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 665\n",
      "Epoch 665 | Loss: 0.42027 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 666\n",
      "Epoch 666 | Loss: 0.41903 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 667\n",
      "Epoch 667 | Loss: 0.41907 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 668\n",
      "Epoch 668 | Loss: 0.41882 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 669\n",
      "Epoch 669 | Loss: 0.41849 | Max grad norm: \n",
      "Time taken: 0.82s\n",
      "\n",
      "Start of epoch 670\n",
      "Epoch 670 | Loss: 0.41806 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 671\n",
      "Epoch 671 | Loss: 0.41711 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 672\n",
      "Epoch 672 | Loss: 0.41678 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 673\n",
      "Epoch 673 | Loss: 0.41730 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 674\n",
      "Epoch 674 | Loss: 0.41606 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 675\n",
      "Epoch 675 | Loss: 0.41617 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 676\n",
      "Epoch 676 | Loss: 0.41549 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 677\n",
      "Epoch 677 | Loss: 0.41493 | Max grad norm: \n",
      "Time taken: 0.78s\n",
      "\n",
      "Start of epoch 678\n",
      "Epoch 678 | Loss: 0.41438 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 679\n",
      "Epoch 679 | Loss: 0.41434 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 680\n",
      "Epoch 680 | Loss: 0.41386 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 681\n",
      "Epoch 681 | Loss: 0.41350 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 682\n",
      "Epoch 682 | Loss: 0.41296 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 683\n",
      "Epoch 683 | Loss: 0.41237 | Max grad norm: \n",
      "Time taken: 0.80s\n",
      "\n",
      "Start of epoch 684\n",
      "Epoch 684 | Loss: 0.41199 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 685\n",
      "Epoch 685 | Loss: 0.41131 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 686\n",
      "Epoch 686 | Loss: 0.41101 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 687\n",
      "Epoch 687 | Loss: 0.41044 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 688\n",
      "Epoch 688 | Loss: 0.41061 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 689\n",
      "Epoch 689 | Loss: 0.41018 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 690\n",
      "Epoch 690 | Loss: 0.40917 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 691\n",
      "Epoch 691 | Loss: 0.40907 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 692\n",
      "Epoch 692 | Loss: 0.40845 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 693\n",
      "Epoch 693 | Loss: 0.40831 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 694\n",
      "Epoch 694 | Loss: 0.40776 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 695\n",
      "Epoch 695 | Loss: 0.40752 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 696\n",
      "Epoch 696 | Loss: 0.40709 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 697\n",
      "Epoch 697 | Loss: 0.40699 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 698\n",
      "Epoch 698 | Loss: 0.40641 | Max grad norm: \n",
      "Time taken: 0.56s\n",
      "\n",
      "Start of epoch 699\n",
      "Epoch 699 | Loss: 0.40555 | Max grad norm: \n",
      "Time taken: 0.62s\n",
      "\n",
      "Start of epoch 700\n",
      "Epoch 700 | Loss: 0.40542 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 701\n",
      "Epoch 701 | Loss: 0.40537 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 702\n",
      "Epoch 702 | Loss: 0.40441 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 703\n",
      "Epoch 703 | Loss: 0.40447 | Max grad norm: \n",
      "Time taken: 0.62s\n",
      "\n",
      "Start of epoch 704\n",
      "Epoch 704 | Loss: 0.40372 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 705\n",
      "Epoch 705 | Loss: 0.40356 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 706\n",
      "Epoch 706 | Loss: 0.40243 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 707\n",
      "Epoch 707 | Loss: 0.40313 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 708\n",
      "Epoch 708 | Loss: 0.40210 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 709\n",
      "Epoch 709 | Loss: 0.40153 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 710\n",
      "Epoch 710 | Loss: 0.40228 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 711\n",
      "Epoch 711 | Loss: 0.40110 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 712\n",
      "Epoch 712 | Loss: 0.40077 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 713\n",
      "Epoch 713 | Loss: 0.39982 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 714\n",
      "Epoch 714 | Loss: 0.40044 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 715\n",
      "Epoch 715 | Loss: 0.39962 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 716\n",
      "Epoch 716 | Loss: 0.39915 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 717\n",
      "Epoch 717 | Loss: 0.39859 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 718\n",
      "Epoch 718 | Loss: 0.39844 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 719\n",
      "Epoch 719 | Loss: 0.39714 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 720\n",
      "Epoch 720 | Loss: 0.39771 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 721\n",
      "Epoch 721 | Loss: 0.39704 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 722\n",
      "Epoch 722 | Loss: 0.39639 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 723\n",
      "Epoch 723 | Loss: 0.39689 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 724\n",
      "Epoch 724 | Loss: 0.39657 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 725\n",
      "Epoch 725 | Loss: 0.39518 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 726\n",
      "Epoch 726 | Loss: 0.39538 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 727\n",
      "Epoch 727 | Loss: 0.39503 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 728\n",
      "Epoch 728 | Loss: 0.39426 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 729\n",
      "Epoch 729 | Loss: 0.39410 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 730\n",
      "Epoch 730 | Loss: 0.39419 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 731\n",
      "Epoch 731 | Loss: 0.39379 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 732\n",
      "Epoch 732 | Loss: 0.39305 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 733\n",
      "Epoch 733 | Loss: 0.39305 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 734\n",
      "Epoch 734 | Loss: 0.39265 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 735\n",
      "Epoch 735 | Loss: 0.39199 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 736\n",
      "Epoch 736 | Loss: 0.39172 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 737\n",
      "Epoch 737 | Loss: 0.39127 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 738\n",
      "Epoch 738 | Loss: 0.39103 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 739\n",
      "Epoch 739 | Loss: 0.39082 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 740\n",
      "Epoch 740 | Loss: 0.38996 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 741\n",
      "Epoch 741 | Loss: 0.39025 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 742\n",
      "Epoch 742 | Loss: 0.38935 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 743\n",
      "Epoch 743 | Loss: 0.38855 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 744\n",
      "Epoch 744 | Loss: 0.38907 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 745\n",
      "Epoch 745 | Loss: 0.38877 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 746\n",
      "Epoch 746 | Loss: 0.38804 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 747\n",
      "Epoch 747 | Loss: 0.38786 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 748\n",
      "Epoch 748 | Loss: 0.38749 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 749\n",
      "Epoch 749 | Loss: 0.38684 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 750\n",
      "Epoch 750 | Loss: 0.38656 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 751\n",
      "Epoch 751 | Loss: 0.38632 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 752\n",
      "Epoch 752 | Loss: 0.38585 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 753\n",
      "Epoch 753 | Loss: 0.38515 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 754\n",
      "Epoch 754 | Loss: 0.38548 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 755\n",
      "Epoch 755 | Loss: 0.38462 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 756\n",
      "Epoch 756 | Loss: 0.38461 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 757\n",
      "Epoch 757 | Loss: 0.38440 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 758\n",
      "Epoch 758 | Loss: 0.38370 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 759\n",
      "Epoch 759 | Loss: 0.38386 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 760\n",
      "Epoch 760 | Loss: 0.38295 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 761\n",
      "Epoch 761 | Loss: 0.38307 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 762\n",
      "Epoch 762 | Loss: 0.38223 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 763\n",
      "Epoch 763 | Loss: 0.38205 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 764\n",
      "Epoch 764 | Loss: 0.38169 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 765\n",
      "Epoch 765 | Loss: 0.38187 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 766\n",
      "Epoch 766 | Loss: 0.38105 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 767\n",
      "Epoch 767 | Loss: 0.38078 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 768\n",
      "Epoch 768 | Loss: 0.38009 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 769\n",
      "Epoch 769 | Loss: 0.37978 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 770\n",
      "Epoch 770 | Loss: 0.37961 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 771\n",
      "Epoch 771 | Loss: 0.37967 | Max grad norm: \n",
      "Time taken: 0.84s\n",
      "\n",
      "Start of epoch 772\n",
      "Epoch 772 | Loss: 0.37894 | Max grad norm: \n",
      "Time taken: 0.81s\n",
      "\n",
      "Start of epoch 773\n",
      "Epoch 773 | Loss: 0.37868 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 774\n",
      "Epoch 774 | Loss: 0.37882 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 775\n",
      "Epoch 775 | Loss: 0.37798 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 776\n",
      "Epoch 776 | Loss: 0.37750 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 777\n",
      "Epoch 777 | Loss: 0.37737 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 778\n",
      "Epoch 778 | Loss: 0.37714 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 779\n",
      "Epoch 779 | Loss: 0.37643 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 780\n",
      "Epoch 780 | Loss: 0.37592 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 781\n",
      "Epoch 781 | Loss: 0.37582 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 782\n",
      "Epoch 782 | Loss: 0.37502 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 783\n",
      "Epoch 783 | Loss: 0.37531 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 784\n",
      "Epoch 784 | Loss: 0.37522 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 785\n",
      "Epoch 785 | Loss: 0.37475 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 786\n",
      "Epoch 786 | Loss: 0.37457 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 787\n",
      "Epoch 787 | Loss: 0.37414 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 788\n",
      "Epoch 788 | Loss: 0.37408 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 789\n",
      "Epoch 789 | Loss: 0.37338 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 790\n",
      "Epoch 790 | Loss: 0.37273 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 791\n",
      "Epoch 791 | Loss: 0.37278 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 792\n",
      "Epoch 792 | Loss: 0.37211 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 793\n",
      "Epoch 793 | Loss: 0.37263 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 794\n",
      "Epoch 794 | Loss: 0.37210 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 795\n",
      "Epoch 795 | Loss: 0.37148 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 796\n",
      "Epoch 796 | Loss: 0.37130 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 797\n",
      "Epoch 797 | Loss: 0.37074 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 798\n",
      "Epoch 798 | Loss: 0.37097 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 799\n",
      "Epoch 799 | Loss: 0.37018 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 800\n",
      "Epoch 800 | Loss: 0.36956 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 801\n",
      "Epoch 801 | Loss: 0.36966 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 802\n",
      "Epoch 802 | Loss: 0.37000 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 803\n",
      "Epoch 803 | Loss: 0.36915 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 804\n",
      "Epoch 804 | Loss: 0.36884 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 805\n",
      "Epoch 805 | Loss: 0.36834 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 806\n",
      "Epoch 806 | Loss: 0.36775 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 807\n",
      "Epoch 807 | Loss: 0.36821 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 808\n",
      "Epoch 808 | Loss: 0.36779 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 809\n",
      "Epoch 809 | Loss: 0.36738 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 810\n",
      "Epoch 810 | Loss: 0.36666 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 811\n",
      "Epoch 811 | Loss: 0.36657 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 812\n",
      "Epoch 812 | Loss: 0.36593 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 813\n",
      "Epoch 813 | Loss: 0.36594 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 814\n",
      "Epoch 814 | Loss: 0.36563 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 815\n",
      "Epoch 815 | Loss: 0.36547 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 816\n",
      "Epoch 816 | Loss: 0.36472 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 817\n",
      "Epoch 817 | Loss: 0.36418 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 818\n",
      "Epoch 818 | Loss: 0.36415 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 819\n",
      "Epoch 819 | Loss: 0.36375 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 820\n",
      "Epoch 820 | Loss: 0.36331 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 821\n",
      "Epoch 821 | Loss: 0.36323 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 822\n",
      "Epoch 822 | Loss: 0.36269 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 823\n",
      "Epoch 823 | Loss: 0.36266 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 824\n",
      "Epoch 824 | Loss: 0.36233 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 825\n",
      "Epoch 825 | Loss: 0.36252 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 826\n",
      "Epoch 826 | Loss: 0.36107 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 827\n",
      "Epoch 827 | Loss: 0.36175 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 828\n",
      "Epoch 828 | Loss: 0.36062 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 829\n",
      "Epoch 829 | Loss: 0.36088 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 830\n",
      "Epoch 830 | Loss: 0.36134 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 831\n",
      "Epoch 831 | Loss: 0.36045 | Max grad norm: \n",
      "Time taken: 0.49s\n",
      "\n",
      "Start of epoch 832\n",
      "Epoch 832 | Loss: 0.36001 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 833\n",
      "Epoch 833 | Loss: 0.35951 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 834\n",
      "Epoch 834 | Loss: 0.35932 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 835\n",
      "Epoch 835 | Loss: 0.35934 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 836\n",
      "Epoch 836 | Loss: 0.35893 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 837\n",
      "Epoch 837 | Loss: 0.35867 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 838\n",
      "Epoch 838 | Loss: 0.35820 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 839\n",
      "Epoch 839 | Loss: 0.35776 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 840\n",
      "Epoch 840 | Loss: 0.35792 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 841\n",
      "Epoch 841 | Loss: 0.35758 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 842\n",
      "Epoch 842 | Loss: 0.35683 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 843\n",
      "Epoch 843 | Loss: 0.35684 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 844\n",
      "Epoch 844 | Loss: 0.35691 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 845\n",
      "Epoch 845 | Loss: 0.35575 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 846\n",
      "Epoch 846 | Loss: 0.35595 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 847\n",
      "Epoch 847 | Loss: 0.35519 | Max grad norm: \n",
      "Time taken: 0.52s\n",
      "\n",
      "Start of epoch 848\n",
      "Epoch 848 | Loss: 0.35503 | Max grad norm: \n",
      "Time taken: 0.61s\n",
      "\n",
      "Start of epoch 849\n",
      "Epoch 849 | Loss: 0.35517 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 850\n",
      "Epoch 850 | Loss: 0.35417 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 851\n",
      "Epoch 851 | Loss: 0.35463 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 852\n",
      "Epoch 852 | Loss: 0.35381 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 853\n",
      "Epoch 853 | Loss: 0.35404 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 854\n",
      "Epoch 854 | Loss: 0.35369 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 855\n",
      "Epoch 855 | Loss: 0.35312 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 856\n",
      "Epoch 856 | Loss: 0.35273 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 857\n",
      "Epoch 857 | Loss: 0.35295 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 858\n",
      "Epoch 858 | Loss: 0.35224 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 859\n",
      "Epoch 859 | Loss: 0.35215 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 860\n",
      "Epoch 860 | Loss: 0.35183 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 861\n",
      "Epoch 861 | Loss: 0.35171 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 862\n",
      "Epoch 862 | Loss: 0.35143 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 863\n",
      "Epoch 863 | Loss: 0.35139 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 864\n",
      "Epoch 864 | Loss: 0.35101 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 865\n",
      "Epoch 865 | Loss: 0.35056 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 866\n",
      "Epoch 866 | Loss: 0.35050 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 867\n",
      "Epoch 867 | Loss: 0.35012 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 868\n",
      "Epoch 868 | Loss: 0.34946 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 869\n",
      "Epoch 869 | Loss: 0.34969 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 870\n",
      "Epoch 870 | Loss: 0.34913 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 871\n",
      "Epoch 871 | Loss: 0.34861 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 872\n",
      "Epoch 872 | Loss: 0.34898 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 873\n",
      "Epoch 873 | Loss: 0.34826 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 874\n",
      "Epoch 874 | Loss: 0.34848 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 875\n",
      "Epoch 875 | Loss: 0.34794 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 876\n",
      "Epoch 876 | Loss: 0.34746 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 877\n",
      "Epoch 877 | Loss: 0.34683 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 878\n",
      "Epoch 878 | Loss: 0.34752 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 879\n",
      "Epoch 879 | Loss: 0.34694 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 880\n",
      "Epoch 880 | Loss: 0.34664 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 881\n",
      "Epoch 881 | Loss: 0.34568 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 882\n",
      "Epoch 882 | Loss: 0.34621 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 883\n",
      "Epoch 883 | Loss: 0.34548 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 884\n",
      "Epoch 884 | Loss: 0.34553 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 885\n",
      "Epoch 885 | Loss: 0.34549 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 886\n",
      "Epoch 886 | Loss: 0.34491 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 887\n",
      "Epoch 887 | Loss: 0.34452 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 888\n",
      "Epoch 888 | Loss: 0.34392 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 889\n",
      "Epoch 889 | Loss: 0.34399 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 890\n",
      "Epoch 890 | Loss: 0.34384 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 891\n",
      "Epoch 891 | Loss: 0.34366 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 892\n",
      "Epoch 892 | Loss: 0.34365 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 893\n",
      "Epoch 893 | Loss: 0.34353 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 894\n",
      "Epoch 894 | Loss: 0.34307 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 895\n",
      "Epoch 895 | Loss: 0.34292 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 896\n",
      "Epoch 896 | Loss: 0.34247 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 897\n",
      "Epoch 897 | Loss: 0.34217 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 898\n",
      "Epoch 898 | Loss: 0.34141 | Max grad norm: \n",
      "Time taken: 0.59s\n",
      "\n",
      "Start of epoch 899\n",
      "Epoch 899 | Loss: 0.34137 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 900\n",
      "Epoch 900 | Loss: 0.34123 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 901\n",
      "Epoch 901 | Loss: 0.34090 | Max grad norm: \n",
      "Time taken: 0.67s\n",
      "\n",
      "Start of epoch 902\n",
      "Epoch 902 | Loss: 0.34076 | Max grad norm: \n",
      "Time taken: 0.62s\n",
      "\n",
      "Start of epoch 903\n",
      "Epoch 903 | Loss: 0.34025 | Max grad norm: \n",
      "Time taken: 0.55s\n",
      "\n",
      "Start of epoch 904\n",
      "Epoch 904 | Loss: 0.34016 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 905\n",
      "Epoch 905 | Loss: 0.33961 | Max grad norm: \n",
      "Time taken: 0.66s\n",
      "\n",
      "Start of epoch 906\n",
      "Epoch 906 | Loss: 0.33950 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 907\n",
      "Epoch 907 | Loss: 0.33952 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 908\n",
      "Epoch 908 | Loss: 0.33924 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 909\n",
      "Epoch 909 | Loss: 0.33884 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 910\n",
      "Epoch 910 | Loss: 0.33879 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 911\n",
      "Epoch 911 | Loss: 0.33837 | Max grad norm: \n",
      "Time taken: 0.82s\n",
      "\n",
      "Start of epoch 912\n",
      "Epoch 912 | Loss: 0.33868 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 913\n",
      "Epoch 913 | Loss: 0.33791 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 914\n",
      "Epoch 914 | Loss: 0.33785 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 915\n",
      "Epoch 915 | Loss: 0.33746 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 916\n",
      "Epoch 916 | Loss: 0.33714 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 917\n",
      "Epoch 917 | Loss: 0.33715 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 918\n",
      "Epoch 918 | Loss: 0.33657 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 919\n",
      "Epoch 919 | Loss: 0.33643 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 920\n",
      "Epoch 920 | Loss: 0.33573 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 921\n",
      "Epoch 921 | Loss: 0.33598 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 922\n",
      "Epoch 922 | Loss: 0.33570 | Max grad norm: \n",
      "Time taken: 0.79s\n",
      "\n",
      "Start of epoch 923\n",
      "Epoch 923 | Loss: 0.33484 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 924\n",
      "Epoch 924 | Loss: 0.33507 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 925\n",
      "Epoch 925 | Loss: 0.33545 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 926\n",
      "Epoch 926 | Loss: 0.33441 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 927\n",
      "Epoch 927 | Loss: 0.33431 | Max grad norm: \n",
      "Time taken: 0.65s\n",
      "\n",
      "Start of epoch 928\n",
      "Epoch 928 | Loss: 0.33410 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 929\n",
      "Epoch 929 | Loss: 0.33422 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 930\n",
      "Epoch 930 | Loss: 0.33389 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 931\n",
      "Epoch 931 | Loss: 0.33336 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 932\n",
      "Epoch 932 | Loss: 0.33322 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 933\n",
      "Epoch 933 | Loss: 0.33313 | Max grad norm: \n",
      "Time taken: 0.68s\n",
      "\n",
      "Start of epoch 934\n",
      "Epoch 934 | Loss: 0.33288 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 935\n",
      "Epoch 935 | Loss: 0.33242 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 936\n",
      "Epoch 936 | Loss: 0.33246 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 937\n",
      "Epoch 937 | Loss: 0.33188 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 938\n",
      "Epoch 938 | Loss: 0.33178 | Max grad norm: \n",
      "Time taken: 0.80s\n",
      "\n",
      "Start of epoch 939\n",
      "Epoch 939 | Loss: 0.33187 | Max grad norm: \n",
      "Time taken: 0.83s\n",
      "\n",
      "Start of epoch 940\n",
      "Epoch 940 | Loss: 0.33119 | Max grad norm: \n",
      "Time taken: 0.75s\n",
      "\n",
      "Start of epoch 941\n",
      "Epoch 941 | Loss: 0.33134 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 942\n",
      "Epoch 942 | Loss: 0.33109 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 943\n",
      "Epoch 943 | Loss: 0.33065 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 944\n",
      "Epoch 944 | Loss: 0.33048 | Max grad norm: \n",
      "Time taken: 0.78s\n",
      "\n",
      "Start of epoch 945\n",
      "Epoch 945 | Loss: 0.33024 | Max grad norm: \n",
      "Time taken: 0.82s\n",
      "\n",
      "Start of epoch 946\n",
      "Epoch 946 | Loss: 0.33000 | Max grad norm: \n",
      "Time taken: 0.77s\n",
      "\n",
      "Start of epoch 947\n",
      "Epoch 947 | Loss: 0.32928 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 948\n",
      "Epoch 948 | Loss: 0.32921 | Max grad norm: \n",
      "Time taken: 0.64s\n",
      "\n",
      "Start of epoch 949\n",
      "Epoch 949 | Loss: 0.32924 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 950\n",
      "Epoch 950 | Loss: 0.32921 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 951\n",
      "Epoch 951 | Loss: 0.32853 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 952\n",
      "Epoch 952 | Loss: 0.32855 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 953\n",
      "Epoch 953 | Loss: 0.32842 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 954\n",
      "Epoch 954 | Loss: 0.32801 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 955\n",
      "Epoch 955 | Loss: 0.32774 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 956\n",
      "Epoch 956 | Loss: 0.32758 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 957\n",
      "Epoch 957 | Loss: 0.32698 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 958\n",
      "Epoch 958 | Loss: 0.32711 | Max grad norm: \n",
      "Time taken: 0.74s\n",
      "\n",
      "Start of epoch 959\n",
      "Epoch 959 | Loss: 0.32664 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 960\n",
      "Epoch 960 | Loss: 0.32687 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 961\n",
      "Epoch 961 | Loss: 0.32622 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 962\n",
      "Epoch 962 | Loss: 0.32645 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 963\n",
      "Epoch 963 | Loss: 0.32634 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 964\n",
      "Epoch 964 | Loss: 0.32586 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 965\n",
      "Epoch 965 | Loss: 0.32529 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 966\n",
      "Epoch 966 | Loss: 0.32564 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 967\n",
      "Epoch 967 | Loss: 0.32483 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 968\n",
      "Epoch 968 | Loss: 0.32482 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 969\n",
      "Epoch 969 | Loss: 0.32458 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 970\n",
      "Epoch 970 | Loss: 0.32408 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 971\n",
      "Epoch 971 | Loss: 0.32415 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 972\n",
      "Epoch 972 | Loss: 0.32340 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 973\n",
      "Epoch 973 | Loss: 0.32351 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 974\n",
      "Epoch 974 | Loss: 0.32377 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 975\n",
      "Epoch 975 | Loss: 0.32336 | Max grad norm: \n",
      "Time taken: 0.76s\n",
      "\n",
      "Start of epoch 976\n",
      "Epoch 976 | Loss: 0.32286 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 977\n",
      "Epoch 977 | Loss: 0.32294 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 978\n",
      "Epoch 978 | Loss: 0.32252 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 979\n",
      "Epoch 979 | Loss: 0.32269 | Max grad norm: \n",
      "Time taken: 0.69s\n",
      "\n",
      "Start of epoch 980\n",
      "Epoch 980 | Loss: 0.32223 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 981\n",
      "Epoch 981 | Loss: 0.32176 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 982\n",
      "Epoch 982 | Loss: 0.32183 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 983\n",
      "Epoch 983 | Loss: 0.32140 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 984\n",
      "Epoch 984 | Loss: 0.32114 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 985\n",
      "Epoch 985 | Loss: 0.32084 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 986\n",
      "Epoch 986 | Loss: 0.32061 | Max grad norm: \n",
      "Time taken: 0.72s\n",
      "\n",
      "Start of epoch 987\n",
      "Epoch 987 | Loss: 0.32025 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 988\n",
      "Epoch 988 | Loss: 0.32021 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 989\n",
      "Epoch 989 | Loss: 0.32032 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 990\n",
      "Epoch 990 | Loss: 0.31963 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 991\n",
      "Epoch 991 | Loss: 0.31965 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 992\n",
      "Epoch 992 | Loss: 0.31958 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 993\n",
      "Epoch 993 | Loss: 0.31930 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 994\n",
      "Epoch 994 | Loss: 0.31941 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 995\n",
      "Epoch 995 | Loss: 0.31880 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 996\n",
      "Epoch 996 | Loss: 0.31896 | Max grad norm: \n",
      "Time taken: 0.71s\n",
      "\n",
      "Start of epoch 997\n",
      "Epoch 997 | Loss: 0.31856 | Max grad norm: \n",
      "Time taken: 0.79s\n",
      "\n",
      "Start of epoch 998\n",
      "Epoch 998 | Loss: 0.31820 | Max grad norm: \n",
      "Time taken: 0.73s\n",
      "\n",
      "Start of epoch 999\n",
      "Epoch 999 | Loss: 0.31804 | Max grad norm: \n",
      "Time taken: 0.70s\n",
      "\n",
      "Start of epoch 1000\n",
      "Epoch 1000 | Loss: 0.31749 | Max grad norm: \n",
      "Time taken: 0.70s\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n",
    "\n",
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(reshape_dataset)\n",
    "    .shuffle(dataset_size, reshuffle_each_iteration=True) # reshuffle the entire dataset\n",
    "    .batch(1)\n",
    ")\n",
    "\n",
    "encoder = autosetup.ComplexEncoder([45, latent_dim])\n",
    "decoder = autosetup.ComplexDecoder([45, sample_shape])\n",
    "\n",
    "# initialize layers: on first call their shape is set based on the shapes of the first data\n",
    "dummy = tf.zeros((1, sample_shape), dtype=tf.complex64)\n",
    "_ = decoder(encoder(dummy))\n",
    "\n",
    "print(type(train_dataset))\n",
    "\n",
    "optimizer = Complex_SGD()\n",
    "alpha = 1e-4 # initial value\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nStart of epoch {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    count = 0\n",
    "    epoch_loss = 0.0\n",
    "    for x in train_dataset:\n",
    "        loss, alpha = train_step(x, alpha, encoder, decoder, optimizer)\n",
    "\n",
    "        count += 1 # this is a bit ugly\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        if count % (dataset_size/10) == 0:\n",
    "            print(\n",
    "                \"Training loss (for one sample) at sample %d: %.4f\"\n",
    "                % (count, float(loss))\n",
    "            )\n",
    "\n",
    "    avg_loss = epoch_loss/(dataset_size)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.5f} | Max grad norm: \") # {max_grad_norm:.2f} TODO\n",
    "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "962d3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the encoder weights (not needed for least squares but helpful in generating examples)\n",
    "wrap_encoder = autosetup.EncoderSave(encoder)\n",
    "_ = wrap_encoder(tf.zeros((1, sample_shape), dtype = tf.complex64))\n",
    "\n",
    "wrap_encoder.save_weights(\"encoder_1000epochs_Xpty_k30.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c494670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the decoder weights\n",
    "wrap_decoder = autosetup.DecoderSave(decoder)\n",
    "_ = wrap_decoder(tf.zeros((1, latent_dim), dtype = tf.complex64))\n",
    "\n",
    "wrap_decoder.save_weights(\"decoder_1000epochs_Xpty_k30_epochs1000.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the encoder weights\n",
    "encoder_from_load = autosetup.ComplexEncoder([50, latent_dim])\n",
    "encoder_model_from_load = autosetup.EncoderSave(encoder_from_load)\n",
    "\n",
    "## load the decoder weights\n",
    "decoder_from_load = autosetup.ComplexDecoder([50, sample_shape])\n",
    "model_from_load = autosetup.DecoderSave(decoder_from_load)\n",
    "\n",
    "# build\n",
    "dummy_enc = tf.zeros((1, sample_shape), dtype = tf.complex64)\n",
    "_ = encoder_from_load(dummy_enc)\n",
    "_ = encoder_model_from_load(dummy_enc)\n",
    "\n",
    "dummy_dec = tf.zeros((1, latent_dim), dtype = tf.complex64)\n",
    "_ = decoder_from_load(dummy_dec)\n",
    "_ = model_from_load(tf.zeros((1, latent_dim), dtype = tf.complex64))\n",
    "\n",
    "# load\n",
    "#model_from_load.load_weights(\"saved_decoder_test.weights.h5\")\n",
    "model_from_load.load_weights(\"decoder_8epochs_x_cx_small.weights.h5\")\n",
    "#encoder_model_from_load.load_weights(\"encoder_8epochs_x_cx_small.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe50fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(\n",
      "[[1.0560035 -7.74386898e-02j 1.0573239 +8.20555985e-02j\n",
      "  1.1151439 -9.14341062e-02j 1.0566394 -3.52024026e-02j\n",
      "  0.93093467-7.01772124e-02j 0.98010314-1.08131403e-02j\n",
      "  1.0116947 -6.06522523e-02j 1.0721003 +2.21436489e-02j\n",
      "  0.9977986 +7.54325837e-02j 1.1252183 -9.12407115e-02j\n",
      "  0.98584217+7.65499771e-02j 1.1194189 -3.10081970e-02j\n",
      "  1.0479074 +8.06405470e-02j 1.0364313 -1.06292199e-02j\n",
      "  0.9330173 +1.00438990e-01j 1.053675  +2.20988348e-01j\n",
      "  1.0738726 +2.39383459e-01j 1.0857968 +4.43139449e-02j\n",
      "  1.03635   -7.87219480e-02j 0.9503862 -4.71685864e-02j\n",
      "  0.9079056 -1.49019165e-02j 0.8564091 -3.92761752e-02j\n",
      "  0.9362015 +5.44368252e-02j 0.9575463 +1.76161274e-01j\n",
      "  1.0529252 +5.25073826e-01j 1.0500501 +6.51388526e-01j\n",
      "  1.0366496 +5.24619102e-01j 1.1171299 +3.76128852e-01j\n",
      "  0.96216846+6.17846064e-02j 1.0660409 +8.73341635e-02j\n",
      "  1.0847453 -5.12152584e-03j 1.0241363 +4.79821227e-02j\n",
      "  0.9760021 +1.58020541e-01j 1.0814556 +2.91096598e-01j\n",
      "  1.0535152 +5.55166006e-01j 0.9526315 +2.97709435e-01j\n",
      "  1.0267683 +3.28449428e-01j 1.0330402 +3.21560681e-01j\n",
      "  0.98320305-9.89362523e-02j 1.0880387 -6.31687865e-02j\n",
      "  1.0526031 -1.23771448e-02j 0.9839762 -6.53552450e-03j\n",
      "  1.0246439 +4.25742827e-02j 1.11533   +1.73348099e-01j\n",
      "  0.9749657 +3.69154036e-01j 1.113561  +3.44498783e-01j\n",
      "  0.95758647+4.59754229e-01j 1.1428967 +2.44592279e-01j\n",
      "  0.99573797+1.53462244e-02j 1.0015408 -3.12598161e-02j\n",
      "  1.0500426 +4.51379083e-02j 1.087172  -1.04521774e-02j\n",
      "  0.9759435 +2.05443874e-01j 1.0085222 +3.04104835e-01j\n",
      "  1.0280935 +3.68622541e-01j 1.0485847 +6.11569881e-01j\n",
      "  0.9671645 +4.82455552e-01j 1.0878669 +2.31826872e-01j\n",
      "  1.0816195 -4.65694889e-02j 1.0287117 -6.50902241e-02j\n",
      "  0.9866466 +1.79192815e-02j 1.057642  -5.85351847e-02j\n",
      "  1.0112156 +1.78291842e-01j 0.9980824 +2.88504928e-01j\n",
      "  1.1203616 +2.45235622e-01j 1.0909275 +3.38759243e-01j\n",
      "  1.0355871 +6.25705719e-01j 1.059353  +2.43134424e-01j\n",
      "  0.93053883+1.15311537e-02j 1.0168926 -7.30931060e-03j\n",
      "  0.85039043+4.57549356e-02j 0.95495594+5.30015714e-02j\n",
      "  0.968765  +2.96884030e-01j 0.942666  +3.41189563e-01j\n",
      "  1.0291957 +6.09030545e-01j 1.0264461 +6.50605857e-01j\n",
      "  1.0422275 +2.74099499e-01j 1.0353549 +1.65900618e-01j\n",
      "  1.0780534 -2.67867129e-02j 1.0923184 +4.34010960e-02j\n",
      "  1.0675977 +8.24682638e-02j 1.0707918 +2.50325352e-02j\n",
      "  1.0454168 +2.74871737e-01j 1.0811124 +2.67751455e-01j\n",
      "  1.0067972 +3.16208273e-01j 1.0304202 +1.25436127e-01j\n",
      "  0.9461455 +1.04200691e-01j 1.0102051 -3.99327241e-02j\n",
      "  0.984258  -8.45163465e-02j 0.84359413-6.75433576e-02j\n",
      "  0.9713124 -1.68332167e-03j 1.0173757 +9.56301298e-03j\n",
      "  0.92174274-2.92138383e-02j 0.9609496 -1.77715123e-02j\n",
      "  0.9054386 +3.03061195e-02j 1.1156515 -3.80987413e-02j\n",
      "  1.0033677 +6.12808503e-02j 1.0021721 -6.07553907e-02j\n",
      "  0.95382047-2.86927359e-04j 0.9473032 -7.51902387e-02j]], shape=(1, 100), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "## test if it does the same\n",
    "x_testing = tf.reshape(x_train_cx_small[0], [1,-1])\n",
    "print(x_testing.shape)\n",
    "print(type(x_testing))\n",
    "y = encoder(x_testing)\n",
    "\n",
    "# if zeros are returned, test succesfull\n",
    "#print(decoder(y) - model_from_load.decoder(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3531ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run if decoder should be loaded from save\n",
    "decoder = model_from_load.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de89a64",
   "metadata": {},
   "source": [
    "#### Extracting and wrapping autoencoder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6cc3a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 64)\n",
      "(1, 8, 8)\n",
      "(1, 64)\n",
      "tf.Tensor(\n",
      "[[ 0.76033205+0.499869j    0.03125612-0.07013683j -0.80597395-0.85858244j\n",
      "  -0.04072828+0.08318315j -0.44001102+0.2534781j  -0.        +0.j\n",
      "  -0.5962155 +0.56593823j  0.39948237-0.8773457j   0.        +0.j\n",
      "  -0.44541728-0.4327705j  -0.04914183+0.01028979j -0.34738308+0.7593429j\n",
      "  -0.2841268 +0.5362347j   0.15344521+0.15696934j  0.11140734-1.9491427j\n",
      "   0.24419409+0.77297854j -0.8544387 -0.06609839j  0.09462023+0.05154117j\n",
      "   0.01554566+0.03732468j -0.20739411+0.01633058j -0.09366388+0.10109431j\n",
      "  -0.11532701-0.01703579j -0.        +0.j         -0.86286026-0.98363626j\n",
      "   0.8854239 +1.2958457j   0.95377666-0.44559228j  0.18710764-0.70623183j\n",
      "  -1.4887398 -2.5469306j   0.49146828-0.06205234j -0.1154036 +0.15555811j]], shape=(1, 30), dtype=complex64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16aa4dfe2e0>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# random input looks like this\n",
    "z_random = np.random.normal(size=(1,30))\n",
    "test_z = decoder(z_random) #test_z is a tf \n",
    "print(test_z.shape)\n",
    "\n",
    "show_z = tf.reshape(test_z, (1,8,8))\n",
    "print(show_z.shape)\n",
    "\n",
    "im_z = tf.math.imag(show_z)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(im_z[0])\n",
    "\n",
    "# an encoded digit looks like this\n",
    "x_testing = tf.reshape(X_tf[-1], [1,-1])\n",
    "print(x_testing.shape)\n",
    "print(y)\n",
    "y = encoder(x_testing)\n",
    "\n",
    "x_recon = decoder(y)\n",
    "im_x = tf.math.imag(tf.reshape(x_recon, (1,nx,nx)))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(im_x[0])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(tf.math.imag(tf.reshape(x_testing, (1,nx,nx)))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac24c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute function generative map\n",
    "def make_tensor_shape(z):\n",
    "    #z = z.reshape((1,-1))\n",
    "    tensor_z = tf.convert_to_tensor(z, dtype=tf.complex64)\n",
    "    return tf.reshape(tensor_z,[1,-1])\n",
    "\n",
    "def make_tensor(z):\n",
    "    return tf.convert_to_tensor(z, dtype=tf.complex64)\n",
    "\n",
    "# wrapper for decoder to numpy function\n",
    "def decoderfunc_1D(z, decoder):\n",
    "    '''\n",
    "    Converts the autoencoder.decoder function into a generative embedding function\n",
    "\n",
    "    input:\n",
    "        z:              np.array of length (latent_dim)\n",
    "        decoder:        custom class autosetup.ComplexDecoder, numpy.array(sample size, latent_dim) --> tensor(sample size, dim^2))    \n",
    "\n",
    "    output:\n",
    "        decoderfunc_1D: function that maps z to numpy array of lenght dim^2 (flattened original image). flatten() ensures shape (n,) and not (1,n)\n",
    "    '''\n",
    "    return decoder(tf.reshape(make_tensor(z), [1,-1])).numpy().flatten()\n",
    "\n",
    "# wrapper for Jacobian computation to numpy function\n",
    "def jac_decoder_1D(z, decoder):\n",
    "    '''\n",
    "    Converts the CBP_decoder computation into a numpy function, outputting the R-derivative of the decoder in z as a numpy array\n",
    "    NOTE: activation function is hardcoded at this point\n",
    "\n",
    "    input:\n",
    "        z:              np.array of length (latent_dim)\n",
    "        decoder:        custom class autosetup.ComplexDecoder, numpy.array(sample size, latent_dim) --> tensor(sample size, dim^2))\n",
    "    '''\n",
    "    dG_dz, _ = CBP_decoder(tf.reshape(make_tensor(z), [1,-1]), decoder, autosetup.Jac_modrelu)\n",
    "    return dG_dz.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57d273",
   "metadata": {},
   "source": [
    "### Integrate with least squares signal optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Op:\n",
    "    def __init__(self, fun, jac, shape):\n",
    "        self.eval = fun\n",
    "        self.jac = jac\n",
    "        self.shape = shape\n",
    "        \n",
    "def objective(z, y, A, G, w=1, lmbda = 0):\n",
    "    \"\"\"\n",
    "    LS objective\n",
    "        (0.5)*\\|A(G(z)) - y\\|_2^2 + 0.5*lmbda**2*\\|w*z\\|_2^2\n",
    "    \"\"\"\n",
    "    if len(y) != A.shape[0]:\n",
    "        print(\"y and A don't match\")\n",
    "        return\n",
    "    if len(z)//2 != G.shape[1]:\n",
    "        print(\"z and G don't match\")\n",
    "        return\n",
    "    if A.shape[1] != G.shape[0]:\n",
    "        print(\"A and G don't match!\")\n",
    "        return\n",
    "    \n",
    "    k  = len(z)//2\n",
    "    zc = z[:k] + 1j*z[k:]\n",
    "    \n",
    "    xc = G.eval(zc)\n",
    "    Dx = G.jac(zc)\n",
    "\n",
    "    print(\"Dx norm\", np.linalg.norm(Dx @ np.ones_like(zc)))\n",
    "    \n",
    "    yp = A.eval(xc)\n",
    "    Dy = A.jac(xc)\n",
    "\n",
    "    #print(\"Dy\", Dy.shape)\n",
    "\n",
    "    val    = (0.5)*np.linalg.norm(yp - y)**2 + (0.5*lmbda**2)*np.linalg.norm(w*zc)**2\n",
    "    gradc  = Dx.H@(Dy.H@(yp - y)) + (lmbda**2)*(w*w)*zc\n",
    "\n",
    "    print(\"val\", val)\n",
    "    #print(\"gradc\", gradc)\n",
    "    \n",
    "    grad  = np.concatenate((np.real(gradc), np.imag(gradc)))\n",
    "    \n",
    "    return val, grad\n",
    "\n",
    "def reconstruct(xtrue, A, G, w=1, sigma=0, lmbda=0):\n",
    "    # sizes\n",
    "    m,n = A.shape\n",
    "    n,k = G.shape\n",
    "    \n",
    "    # generate data\n",
    "    yobs  = A.eval(xtrue) + sigma*np.random.randn(m)\n",
    "\n",
    "    # print(\"xtrue\", xtrue.shape)\n",
    "    # print(\"yobs\", yobs.shape)\n",
    "    print(\"k\", k)\n",
    "    print(\"m\", m)\n",
    "\n",
    "    # inference\n",
    "    #np.concatenate([np.ones(k), 0.5*np.ones(k)])\n",
    "    fake_z0_tf = encoder(make_tensor_shape(xtrue))\n",
    "    fake_z0_cx = fake_z0_tf.numpy().flatten()\n",
    "    fake_z0 = np.concatenate([np.real(fake_z0_cx), np.imag(fake_z0_cx)])\n",
    "\n",
    "    result = minimize(objective, x0=fake_z0, args=(yobs, A, G, w, lmbda), method='L-BFGS-B', jac=True, options={\n",
    "        'maxiter': 10000,      # total outer iterations\n",
    "        'maxls': 40,          # line search steps per iteration\n",
    "        'ftol' : 1e-14,\n",
    "        'gtol': 1e-14,         # gradient tolerance\n",
    "        'disp': True          # print optimization log\n",
    "    })\n",
    "    \n",
    "    # result = minimize(objective, np.concatenate([np.ones(k), 0.5*np.ones(k)]), args=(yobs, A, G, w, lmbda), method='Powell', jac=True, options={\n",
    "    #     'maxiter': 1000,      # total outer iterations\n",
    "    #     'ftol' : 1e-10,\n",
    "    #     'disp': True          # print optimization log\n",
    "    # })\n",
    "\n",
    "    # result\n",
    "    zhat = result.x[:k] + 1j*result.x[k:]\n",
    "    xhat = G.eval(zhat)\n",
    "\n",
    "    print(\"zhat\", zhat)\n",
    "    print(\"xhat\", xhat)\n",
    "\n",
    "    print(\"Result message:\", result.message)\n",
    "    print(\"Result status:\", result.status)\n",
    "    print(\"Function evals:\", result.nfev)\n",
    "    print(\"Jacobian evals:\", result.njev)\n",
    "    print(\"Final gradient norm:\", np.linalg.norm(result.jac))\n",
    "    \n",
    "    # correct global phase\n",
    "    phi = np.mean(np.angle(xtrue/(xhat + 1e-5*np.ones_like(xhat))))\n",
    "    #phi = np.mean(np.angle(xtrue/xhat))\n",
    "    xhat_corr = np.exp(1j*phi)*xhat\n",
    "    \n",
    "    # relative error\n",
    "    error = np.linalg.norm(xhat_corr - xtrue)/np.linalg.norm(xtrue)\n",
    "    \n",
    "    # return\n",
    "    return error, xhat_corr, yobs\n",
    "\n",
    "def plot_result(xtrue, xhat):\n",
    "    n  = len(xtrue)\n",
    "    nx = int(np.sqrt(n))\n",
    "    \n",
    "    # plot results\n",
    "    fig, ax = plt.subplots(2,2)\n",
    "\n",
    "    ax[0,0].imshow(np.real(xtrue.reshape((nx,nx))),clim=[0,1])\n",
    "    ax[0,0].set_title(r'$\\Re(x_{true})$')\n",
    "    ax[1,0].imshow(np.imag(xtrue.reshape((nx,nx))),clim=[0,1])\n",
    "    ax[1,0].set_title(r'$\\Im(x_{true})$')\n",
    "    ax[0,1].imshow(np.real(xhat.reshape((nx,nx))),clim=[0,1])\n",
    "    ax[0,1].set_title(r'$\\Re(x_{est})$')\n",
    "    ax[1,1].imshow(np.imag(xhat.reshape((nx,nx))),clim=[0,1])\n",
    "    ax[1,1].set_title(r'$\\Im(x_{est})$')\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c5f9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskedFourier(M):\n",
    "    \"\"\"\n",
    "    Defined masked 2D fourier transform as linear operator.\n",
    "    \n",
    "    input:\n",
    "        M - 3D array of size n x n x m containing m masks of size n x n\n",
    "        \n",
    "    out:\n",
    "        A - linear operator representing the masked Fourier transforms\n",
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    nx = M.shape[0]\n",
    "    mx = M.shape[2]\n",
    "    \n",
    "    mv  = lambda x : fft2(M*(x.reshape((nx,nx,1))), axes=(0,1)).flatten()\n",
    "    rmv = lambda y : nx*nx*np.sum(np.conj(M)*ifft2(y.reshape((nx,nx,mx)), axes=(0,1)),axis=2).flatten()\n",
    "    A   = LinearOperator((mx*nx*nx, nx*nx), matvec=mv, rmatvec=rmv) # rmatvec is conjugate operation, so A^H * v\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e603f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "mx      = 100       # number of masks\n",
    "n       = sample_shape     # total length of one image\n",
    "nx      = int(np.sqrt(sample_shape))  # one side of the image (only works if sample_shape is a square)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9814a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400, 64)\n"
     ]
    }
   ],
   "source": [
    "# define forward operator with binary masks\n",
    "m  = mx * n\n",
    "\n",
    "M = np.random.randn(nx,nx,mx)\n",
    "M[M<0]=0\n",
    "M[M>0]=1\n",
    "\n",
    "MF   = MaskedFourier(M)\n",
    "print(MF.shape)\n",
    "Afun = lambda x : np.abs(MF@x)**2\n",
    "Ajac = lambda x : LinearOperator((m, n), matvec=lambda z : 2*(MF@x)*np.conj(MF@np.conj(z)), rmatvec=lambda z : 2*(MF.H@((MF@x)*z)))\n",
    "\n",
    "A    = Op(fun = Afun, jac = Ajac, shape=(m,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "013bb903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prior from decoder\n",
    "k               = latent_dim\n",
    "block_identity  = lambda mat : np.concatenate((mat, np.eye(n)), axis=1)\n",
    "\n",
    "\n",
    "#define generative models\n",
    "I = Op(fun = lambda z : z, jac = lambda z: LinearOperator((n, n), matvec = lambda z : z, rmatvec = lambda z : z), shape=(n,n))\n",
    "\n",
    "G = Op(fun = lambda z : decoderfunc_1D(z, decoder), \n",
    "       jac = lambda z : LinearOperator((n,k), matvec = lambda v : jac_decoder_1D(z, decoder) @ v, \n",
    "                                       rmatvec = lambda p : np.conj(jac_decoder_1D(z, decoder).T) @ p),\n",
    "       shape = (n,k))\n",
    "\n",
    "H = Op(fun = lambda z : decoderfunc_1D(z[:k], decoder) + z[k:], \n",
    "       jac = lambda z : LinearOperator((n,k+n), matvec = lambda v : block_identity(jac_decoder_1D(z[:k], decoder)) @ v, \n",
    "                                       rmatvec = lambda p : np.conj(block_identity(jac_decoder_1D(z[:k], decoder)).T) @ p),\n",
    "       shape = (n,k+n))\n",
    "\n",
    "# define weights\n",
    "w = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ec47418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "(100, 10)\n",
      "(10, 100)\n",
      "<class 'scipy.sparse.linalg._interface._CustomLinearOperator'>\n",
      "<class 'scipy.sparse.linalg._interface._CustomLinearOperator'>\n",
      "(100,)\n",
      "(10,)\n",
      "(100,)\n",
      "(100, 110)\n",
      "(110, 100)\n"
     ]
    }
   ],
   "source": [
    "# testing operators\n",
    "z_testing = np.ones(10)\n",
    "x_testing = np.ones(100)\n",
    "# y_testing = np.ones((1000))\n",
    "h_testing = np.ones((110))\n",
    "\n",
    "print(G.eval(z_testing).shape)\n",
    "\n",
    "G_jac = G.jac(z_testing)\n",
    "G_H = G_jac.H\n",
    "\n",
    "print(G_jac.shape)\n",
    "print(G_H.shape)\n",
    "\n",
    "print(type(G_jac))\n",
    "print(type(G_H))\n",
    "\n",
    "G_jac@z_testing\n",
    "print((G_jac@z_testing).shape)\n",
    "\n",
    "G_Hx = G_H @ x_testing\n",
    "print(G_Hx.shape)\n",
    "\n",
    "H_func = H.eval(h_testing)\n",
    "H_jac = H.jac(h_testing)\n",
    "H_H = H_jac.H\n",
    "\n",
    "print(H_func.shape)\n",
    "print(H_jac.shape)\n",
    "print(H_H.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9f719",
   "metadata": {},
   "source": [
    "##### test the functions before applying algorithm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6501943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y (1000,)\n",
      "xc (100,)\n",
      "Dx (100, 10)\n",
      "yp (1000,)\n",
      "Dy (1000, 100)\n",
      "val 27660830.740326907\n",
      "tussenstap (100,)\n",
      "Dx_H (10, 100)\n",
      "tussenstap2 (10, 1000)\n",
      "tussenstap3 (10,)\n",
      "gradc (10,)\n",
      "grad (20,)\n"
     ]
    }
   ],
   "source": [
    "z = np.ones(2*k)\n",
    "w = 1\n",
    "lmbda = 0\n",
    "sigma = 0\n",
    "\n",
    "xtrue = x_test_cx_small[0].numpy()\n",
    "xtrue_1D = xtrue.flatten()\n",
    "\n",
    "y = A.eval(xtrue_1D) + sigma*np.random.randn(m)\n",
    "\n",
    "temp  = len(z)//2\n",
    "zc = z[:temp] + 1j*z[temp:]\n",
    "\n",
    "#print(\"zc\", zc.shape)\n",
    "\n",
    "print(\"y\", y.shape)\n",
    "\n",
    "xc = G.eval(zc)\n",
    "Dx = G.jac(zc)\n",
    "\n",
    "print(\"xc\", xc.shape)\n",
    "print(\"Dx\", Dx.shape)\n",
    "    \n",
    "yp = A.eval(xc)\n",
    "Dy = A.jac(xc)\n",
    "\n",
    "print(\"yp\", yp.shape)\n",
    "print(\"Dy\", Dy.shape)\n",
    "\n",
    "val    = (0.5)*np.linalg.norm(yp - y)**2 + (0.5*lmbda**2)*np.linalg.norm(w*zc)**2    \n",
    "\n",
    "print(\"val\", val)\n",
    "\n",
    "tussenstap = Dy.H @ (yp - y)\n",
    "\n",
    "print(\"tussenstap\", tussenstap.shape)\n",
    "\n",
    "Dx_H = Dx.H\n",
    "print(\"Dx_H\", Dx_H.shape)\n",
    "\n",
    "print(\"tussenstap2\", (Dx.H @ Dy.H).shape)\n",
    "\n",
    "tussenstap3 = (Dx.H @ Dy.H) @ (yp - y)\n",
    "print(\"tussenstap3\", tussenstap3.shape)\n",
    "\n",
    "gradc  = Dx.H@(Dy.H@(yp - y)) + (lmbda**2)*(w*w)*zc\n",
    "\n",
    "print(\"gradc\", gradc.shape)\n",
    "    \n",
    "grad  = np.concatenate((np.real(gradc), np.imag(gradc)))\n",
    "\n",
    "print(\"grad\", grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a3ffba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 30\n",
      "m 64\n",
      "Dx norm 6.920789047710934\n",
      "val 292.7179909634316\n",
      "Dx norm 6.856991173571453\n",
      "val 237.04145679448752\n",
      "Dx norm 6.514118834126115\n",
      "val 76.61370824167966\n",
      "Dx norm 6.456831411795116\n",
      "val 50.19532613807071\n",
      "Dx norm 6.443363877214331\n",
      "val 33.31960128527227\n",
      "Dx norm 6.572390520871141\n",
      "val 29.373678850250347\n",
      "Dx norm 6.534333272281736\n",
      "val 23.17231443504634\n",
      "Dx norm 6.3494703221835955\n",
      "val 19.453922469831504\n",
      "Dx norm 6.288670667272156\n",
      "val 19.339500215112455\n",
      "Dx norm 6.299696306671663\n",
      "val 25.598023948595888\n",
      "Dx norm 6.277699995015237\n",
      "val 19.489340192093632\n",
      "Dx norm 6.286908170080283\n",
      "val 19.35632297069925\n",
      "Dx norm 6.288384562564456\n",
      "val 19.34206801397097\n",
      "Dx norm 6.288624316307021\n",
      "val 19.339915169690716\n",
      "Dx norm 6.288663128219599\n",
      "val 19.33956643818466\n",
      "Dx norm 6.288669355603709\n",
      "val 19.33950517148905\n",
      "Dx norm 6.288670408895704\n",
      "val 19.33949956882455\n",
      "Dx norm 6.288669726499774\n",
      "val 19.33950680960856\n",
      "Dx norm 6.288670422851912\n",
      "val 19.33949997420866\n",
      "Dx norm 6.288670525646992\n",
      "val 19.33950046457204\n",
      "Dx norm 6.288670408895704\n",
      "val 19.33949956882455\n",
      "Dx norm 6.314802458103531\n",
      "val 32.623393664607036\n",
      "Dx norm 6.276030163854019\n",
      "val 19.57773121329033\n",
      "Dx norm 6.286543751018537\n",
      "val 19.36495540866566\n",
      "Dx norm 6.288323372376245\n",
      "val 19.343358316219387\n",
      "Dx norm 6.2886140798522705\n",
      "val 19.340126448554606\n",
      "Dx norm 6.288661208164352\n",
      "val 19.33960024421154\n",
      "Dx norm 6.288668988364846\n",
      "val 19.339513670198425\n",
      "Dx norm 6.288670085927463\n",
      "val 19.33950463602382\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.288670401377848\n",
      "val 19.33950465199984\n",
      "Dx norm 6.288670189000248\n",
      "val 19.33949810145006\n",
      "Dx norm 6.288670103791246\n",
      "val 19.33949916216721\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.400703889120806\n",
      "val 48.203301533977175\n",
      "Dx norm 6.273419813464666\n",
      "val 19.659794515232548\n",
      "Dx norm 6.285308908793401\n",
      "val 19.37072009093423\n",
      "Dx norm 6.288079075115748\n",
      "val 19.344330524122263\n",
      "Dx norm 6.288567165559981\n",
      "val 19.340326336358103\n",
      "Dx norm 6.288652287447236\n",
      "val 19.339642876219546\n",
      "Dx norm 6.288667453445543\n",
      "val 19.33952854421615\n",
      "Dx norm 6.288669837057407\n",
      "val 19.339504036557912\n",
      "Dx norm 6.288670599922723\n",
      "val 19.339502487628348\n",
      "Dx norm 6.288670192741022\n",
      "val 19.339497810915926\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "Dx norm 6.2886702359663\n",
      "val 19.339497728312956\n",
      "zhat [ 1.48955782+1.88604237j  1.25883254-2.26579893j -1.64133125-2.95502863j\n",
      "  0.64436134+1.22782904j -1.47965252+1.37599744j  0.67684306-0.45770085j\n",
      "  0.27701776-0.46109054j  2.37668202-0.15603815j -0.44758442+0.74798902j\n",
      "  1.16559339+0.28830225j -0.57151415+1.56533464j -0.56319491+2.57028634j\n",
      "  1.09216665+3.41379869j  0.92985819+2.73920805j -1.54722786+0.85988604j\n",
      "  0.86843977+1.37787531j -1.10300675-0.48861719j  0.07463119+1.43289652j\n",
      "  2.14124068-0.81693438j  0.58777485-0.30007546j -1.46080458+0.36312166j\n",
      "  0.91325835+2.34143277j -0.95556693+0.64742705j -0.71650617-2.18816021j\n",
      " -2.70366092+1.7409911j   0.93027067+2.34404837j  1.00932115+0.73302257j\n",
      "  1.38096263-3.49641105j -1.53426158+1.6497861j   2.82331365+1.50040801j]\n",
      "xhat [ 1.0771027 -0.64247614j  1.1914802 +0.7157859j   1.6150402 +0.20146967j\n",
      "  1.4974879 +1.2725642j   1.544425  +0.3412441j   2.2749543 +1.3545768j\n",
      " -0.13375881-0.33059826j  0.31070596+0.04185523j  1.0146286 -0.13808592j\n",
      "  0.5767558 -0.31881985j  0.78834766+1.1045383j   1.5654621 +0.7691282j\n",
      "  0.3186676 +0.0717398j   0.9312586 +1.2867606j   1.0677772 +0.35266814j\n",
      "  1.4992173 +0.32134637j  1.4642441 +0.26110032j  0.5160049 -0.08071018j\n",
      "  0.82417184-0.27583385j  0.7890888 +0.1461264j   0.6853552 +0.7506845j\n",
      "  0.86140466-0.03238419j  0.636979  -0.55669504j  0.82215816-0.1006756j\n",
      "  0.82794976+0.9910087j   0.40759486-0.33957312j  0.6386327 +0.27990106j\n",
      "  1.2837436 -0.1608457j   0.3129022 -0.3377432j  -0.3503099 -0.44284353j\n",
      "  0.395897  -0.638089j    0.        -0.j         -0.28564975-0.28845292j\n",
      "  0.5461752 +0.36734977j  0.9521694 +1.1210725j   0.48929572+0.7434177j\n",
      "  1.6805645 +1.4935666j   2.2275627 -0.07533701j  0.46040168+0.7761443j\n",
      "  0.46083617-0.16499348j  1.6342428 +0.19968241j  0.26565802-0.1848141j\n",
      " -0.11973928-0.07511368j  0.94705385-0.8245393j   1.255094  +0.45127773j\n",
      "  2.1007512 +0.13168453j  0.90837765-0.23615831j  0.9689904 -0.07626662j\n",
      "  0.837687  -0.01810873j  0.683848  +0.60864544j  1.3038306 -0.44826186j\n",
      "  1.6552769 +0.3555818j   1.0011299 +0.08358143j  0.11323656+0.37504706j\n",
      "  1.2086039 +0.37123534j  0.5146086 +0.33695245j  1.5801748 -0.1634525j\n",
      "  1.7575088 +0.18352772j  0.8674185 +0.74810016j  0.9921243 -0.1284236j\n",
      "  0.34338453-0.22816408j  1.0042971 +0.51693887j  0.8287896 -0.10436798j\n",
      "  1.3011682 +0.49038348j]\n",
      "Result message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n",
      "Result status: 0\n",
      "Function evals: 60\n",
      "Jacobian evals: 60\n",
      "Final gradient norm: 3.043297172665934\n",
      "0.6855525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16aa678f6d0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test generative model itself\n",
    "xtrue = X_tf[-2].numpy()\n",
    "xtrue_1D = xtrue.flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.imag(xtrue.reshape(nx,nx)))\n",
    "\n",
    "error, xhat_corr, yobs = reconstruct(xtrue_1D, I, G, w=1, sigma = 0, lmbda=0)\n",
    "print(error)\n",
    "\n",
    "xhat_corr = xhat_corr.reshape((nx,nx))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.imag(xhat_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa354a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16a82929e50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test with l2\n",
    "xtrue = x_test_cx_small[0].numpy()\n",
    "xtrue_1D = xtrue.flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.imag(xtrue))\n",
    "\n",
    "error, xhat_corr, yobs = reconstruct(xtrue_1D, A, G, w=1, sigma=0.01, lmbda=0)\n",
    "\n",
    "xhat_corr = xhat_corr.reshape((nx,nx))\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.imag(xhat_corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa7fe53",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a78dd0",
   "metadata": {},
   "source": [
    "##### but first test if my code works for the error plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a139ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_1516\\1002254489.py:69: RuntimeWarning: invalid value encountered in divide\n",
      "  phi = np.mean(np.angle(xtrue/xhat))\n",
      "C:\\Users\\Abel\\AppData\\Local\\Temp\\ipykernel_1516\\1002254489.py:69: RuntimeWarning: divide by zero encountered in divide\n",
      "  phi = np.mean(np.angle(xtrue/xhat))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1516\\3807263837.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0msigmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0merrors_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mxhats_i\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'complex'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0merrors_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mxhats_g\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'complex'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1516\\1002254489.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(xtrue, A, G, w, sigma, lmbda)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# print(\"k\", k)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# print(\"m\", m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# inference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlmbda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'L-BFGS-B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;31m# result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mzhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1j\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m         res = _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    711\u001b[0m                                  **options)\n\u001b[0;32m    712\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[0;32m    714\u001b[0m                                callback=callback, **options)\n\u001b[0;32m    715\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m         res = _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[1;31m# The minimization routine wants f and g at the current x.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;31m# Note that interruptions due to maxfun are postponed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[0mn_iterations\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfun_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnfev\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# Send a copy because the user may overwrite it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[1;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[0mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[1;31m# Make sure the function returns a true scalar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\optimize\\_optimize.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\optimize\\_optimize.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_compute_if_needed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1516\\1002254489.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(z, y, A, G, w, lmbda)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m#print(\"Dy\", Dy.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mval\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myp\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlmbda\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mzc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mgradc\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mDx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m@\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m@\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myp\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlmbda\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mzc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# print(\"val\", val)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# print(\"gradc\", gradc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__matmul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[0;32m    454\u001b[0m                              \"use '*' instead\")\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;31m# Sparse matrices shouldn't be converted to numpy arrays.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dimension mismatch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\scipy\\sparse\\linalg\\_interface.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_matvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__matvec_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1516\\215325387.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(p)\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                                        rmatvec = lambda p : np.conj(block_identity(jac_decoder_1D(z[:k], decoder)).T) @ p),\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1516\\3354511919.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(z, decoder)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mz\u001b[0m\u001b[1;33m:\u001b[0m              \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         decoder:        custom class autosetup.ComplexDecoder, numpy.array(sample size, latent_dim) --> tensor(sample size, dim^2))\n\u001b[0;32m     33\u001b[0m     '''\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mdG_dz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCBP_decoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmake_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mautosetup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJac_modrelu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdG_dz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\Documents\\GitHub\\signal-thesis\\complex autoencoder\\backpropagation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(z, decoder, jac_act)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mda_dq_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda_dql_prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0mda_dqstar_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda_dql_prev_star\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[0mdaL_dz\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mda_dq_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_lprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mda_dqstar_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_lprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m     \u001b[0mdaL_dzstar\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mda_dq_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_lprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mda_dqstar_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_lprev\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdaL_dz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdaL_dzstar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\framework\\override_binary_operator.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_math_operator_overrides.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_matmul_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, name)\u001b[0m\n\u001b[0;32m   3812\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmatmul_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-function-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3813\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_numpy_style_type_promotion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3815\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1268\u001b[1;33m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, grad_a, grad_b, name)\u001b[0m\n\u001b[0;32m   3695\u001b[0m             \u001b[0mgrad_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3696\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3697\u001b[0m         )\n\u001b[0;32m   3698\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3699\u001b[1;33m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[0;32m   3700\u001b[0m             \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3701\u001b[0m             \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3702\u001b[0m             \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Abel\\miniconda3\\envs\\wtf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(a, b, transpose_a, transpose_b, grad_a, grad_b, name)\u001b[0m\n\u001b[0;32m   6233\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6234\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6235\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6236\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6237\u001b[1;33m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6238\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6239\u001b[0m       return mat_mul_eager_fallback(\n\u001b[0;32m   6240\u001b[0m           \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# scan over noise levels with and without generative model on in-distribution gt\n",
    "ns     = 2\n",
    "sigmas = np.logspace(-6,6,10)\n",
    "\n",
    "errors_i = np.zeros((len(sigmas),ns))\n",
    "xhats_i  = np.zeros((len(sigmas),ns, n),dtype='complex')\n",
    "\n",
    "errors_g = np.zeros((len(sigmas),ns))\n",
    "xhats_g  = np.zeros((len(sigmas),ns, n),dtype='complex')\n",
    "\n",
    "errors_c = np.zeros((len(sigmas),ns))\n",
    "xhats_c  = np.zeros((len(sigmas),ns, n),dtype='complex')\n",
    "\n",
    "for i in range(len(sigmas)):\n",
    "    for j in range(ns):\n",
    "        xtrue = G.eval(np.random.randn(k) + 1j*np.random.randn(k)) #here xtrue comes from the generative model\n",
    "\n",
    "        errors_i[i,j], xhats_i[i,j,:], _ = reconstruct(xtrue, A, I, w=1, sigma=sigmas[i], lmbda=sigmas[i])\n",
    "        errors_g[i,j], xhats_g[i,j,:], _ = reconstruct(xtrue, A, G, w=1, sigma=sigmas[i], lmbda=sigmas[i])\n",
    "        errors_c[i,j], xhats_c[i,j,:], _ = reconstruct(xtrue, A, H, w=1, sigma=sigmas[i], lmbda=1e1*sigmas[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
